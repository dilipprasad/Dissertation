{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawling Web Pages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8UcMCcd0Ku68F8T+QFmtH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/Crawling_Web_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T2mT0Grn274_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**"
      ],
      "metadata": {
        "id": "IptUs657qgvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dissertation project for final year\n",
        "\n",
        "\n",
        "---\n",
        "This File Takes care of crawling the given web page for sub urls and pushes those urls to a queue for further steps.\n",
        "\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        "\n",
        "\n",
        "*   Beautiful soup\n",
        "*   Regex\n",
        "*   UrlLib\n",
        "*   Regex\n",
        "\n",
        "Additionally used sys, subprocess, pkg_resources for package installation"
      ],
      "metadata": {
        "id": "yzf52m3bXEnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all the  Packages used in the project if missing"
      ],
      "metadata": {
        "id": "y6Ef2pqTXKBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)"
      ],
      "metadata": {
        "id": "5gzsMcj3W4s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse"
      ],
      "metadata": {
        "id": "_QXfJWgD5uTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup to read the web pages and parse them for NLP Operations**"
      ],
      "metadata": {
        "id": "Z-Vt-QjFqkVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter the Root Url -allow the url to be crawled to find other url and finally get all the contents from all links"
      ],
      "metadata": {
        "id": "jIrH-fCOtmF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Main Link\n",
        "url = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "\n",
        "ignoredLinks = ['https://www.bundesarchiv.de/cocoon/barch/'] #Limiting the sub url crawl with 1 more more root url\n",
        "\n",
        "\n",
        "webPageReadTimeout = 10 #For soup object"
      ],
      "metadata": {
        "id": "lr90vHZUDo3d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intialize the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n"
      ],
      "metadata": {
        "id": "VT46lCUO6IPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "queue_name = \"queue-crawledarchiveurls\"\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "xCAZ2eoh6HMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the Joined correct url for the given sub url, by deauflt the base url is the Starting page to crawl - which could be overridden if required. <br/>\n",
        "\n",
        "JoinUrl()  - Joins Sub url with parent url <br/>\n",
        "IsAbolsuteUrl()  - Returns False in case of  Sub url  <br/>"
      ],
      "metadata": {
        "id": "-7aEJLxzeqRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urljoin\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def JoinUrl(subUrl, baseUrl = url):\n",
        "  print(\"BaseURL\",baseUrl)\n",
        "  print(\"SubUrl\",subUrl)\n",
        "  return urljoin(baseUrl,subUrl)\n",
        "\n",
        "\n",
        "def IsAbsoluteUrl(url):\n",
        "    return bool(urlparse(url).netloc)\n",
        "\n",
        "\n",
        "#Testing Scenario 1\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "#Testing Scenario 2\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de/\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://google.com\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sinAUbMcU1i",
        "outputId": "1360c182-044e-473e-ff47-343040f31e7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IsAbsoluteUrl False\n",
            "BaseURL https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\n",
            "SubUrl ../k/k1983k/index.html\n",
            "Joined Url https://www.bundesarchiv.de/cocoon/barch/0000/k/k1983k/index.html\n",
            "IsAbsoluteUrl False\n",
            "BaseURL https://www.bundesarchiv.de/\n",
            "SubUrl ../k/k1983k/index.html\n",
            "Joined Url https://www.bundesarchiv.de/k/k1983k/index.html\n",
            "IsAbsoluteUrl True\n",
            "IsAbsoluteUrl False\n",
            "IsAbsoluteUrl True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function which returns if it is a valid url using Regex**\n",
        "\n",
        "Source code from \n",
        "[Stack overflow](https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not)"
      ],
      "metadata": {
        "id": "klLXcIYJUyMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create regex\n",
        "import re\n",
        "\n",
        "regex = re.compile(\n",
        "    r\"(\\w+://)?\"                # protocol                      (optional)\n",
        "    r\"(\\w+\\.)?\"                 # host                          (optional)\n",
        "    r\"((\\w+)\\.(\\w+))\"           # domain\n",
        "    r\"(\\.\\w+)*\"                 # top-level domain              (optional, can have > 1)\n",
        "    r\"([\\w\\-\\._\\~/]*)*(?<!\\.)\"  # path, params, anchors, etc.   (optional)\n",
        ")\n",
        "\n",
        "def IsValidUrl_Regex(url):\n",
        "  try: \n",
        "    return regex.match(url).span()[1] - regex.match(url).span()[0] == len(url)\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "cases = [\n",
        "    \"http://www.google.com\",\n",
        "    \"https://www.google.com\",\n",
        "    \"http://google.com\",\n",
        "    \"https://google.com\",\n",
        "    \"www.google.com\",\n",
        "    \"google.com\",\n",
        "    \"http://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"http://google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://google.com/~as_db3.2123/134-1a\",\n",
        "    \"www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"google.com/~as_db3.2123/134-1a\",\n",
        "    # .co.uk top level\n",
        "    \"http://www.google.co.uk\",\n",
        "    \"https://www.google.co.uk\",\n",
        "    \"http://google.co.uk\",\n",
        "    \"https://google.co.uk\",\n",
        "    \"www.google.co.uk\",\n",
        "    \"google.co.uk\",\n",
        "    \"http://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"http://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://...\",\n",
        "    \"https://..\",\n",
        "    \"https://.\",\n",
        "    \"https://.google.com\",\n",
        "    \"https://..google.com\",\n",
        "    \"https://...google.com\",\n",
        "    \"https://.google..com\",\n",
        "    \"https://.google...com\"\n",
        "    \"https://...google..com\",\n",
        "    \"https://...google...com\",\n",
        "    \".google.com\",\n",
        "    \".google.co.\"\n",
        "    \"https://google.co.\"\n",
        "]\n",
        "\n",
        "for c in cases:\n",
        "  print(c, IsValidUrl_Regex(c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7G__bpzU2ED",
        "outputId": "56f6ace2-aef7-4f30-ad99-f4f606713148"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://www.google.com True\n",
            "https://www.google.com True\n",
            "http://google.com True\n",
            "https://google.com True\n",
            "www.google.com True\n",
            "google.com True\n",
            "http://www.google.com/~as_db3.2123/134-1a True\n",
            "https://www.google.com/~as_db3.2123/134-1a True\n",
            "http://google.com/~as_db3.2123/134-1a True\n",
            "https://google.com/~as_db3.2123/134-1a True\n",
            "www.google.com/~as_db3.2123/134-1a True\n",
            "google.com/~as_db3.2123/134-1a True\n",
            "http://www.google.co.uk True\n",
            "https://www.google.co.uk True\n",
            "http://google.co.uk True\n",
            "https://google.co.uk True\n",
            "www.google.co.uk True\n",
            "google.co.uk True\n",
            "http://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "https://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "http://google.co.uk/~as_db3.2123/134-1a True\n",
            "https://google.co.uk/~as_db3.2123/134-1a True\n",
            "www.google.co.uk/~as_db3.2123/134-1a True\n",
            "google.co.uk/~as_db3.2123/134-1a True\n",
            "https://... False\n",
            "https://.. False\n",
            "https://. False\n",
            "https://.google.com False\n",
            "https://..google.com False\n",
            "https://...google.com False\n",
            "https://.google..com False\n",
            "https://.google...comhttps://...google..com False\n",
            "https://...google...com False\n",
            ".google.com False\n",
            ".google.co.https://google.co. False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive function to crawl and find all the valid Urls, for further processing"
      ],
      "metadata": {
        "id": "T4AVWIeIYn2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetValidURL(urlval):\n",
        "  # print(\"GetValidURL\",urlval)\n",
        "  linkToAdd= None\n",
        "  if urlval != None and IsAbsoluteUrl(urlval) == False and urlval[0:1] != \"#\":    \n",
        "    # print(\"GetValidURL-sub url parent\",urlval)\n",
        "    linkToAdd = urlval\n",
        "    if IsAbsoluteUrl(urlval) == False: #If sublink\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -false\",urlval)\n",
        "      linkToAdd = JoinUrl(urlval) #Join with base url and get full path \n",
        "    else:\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -true\",urlval)\n",
        "      linkToAdd = urlval       \n",
        "  elif IsValidUrl_Regex(urlval) and urlval != None: #Only if the link is valid\n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -valid link\",urlval)\n",
        "    linkToAdd = urlval\n",
        "  else: \n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -Not valid link\",urlval)\n",
        "    linkToAdd= None\n",
        "    #print(ctr, \"Not valid based on regex\",nextLink) - Dont do anything, Ignore invalid Urls\n",
        "\n",
        "  return linkToAdd\n",
        "\n",
        "\n",
        " \n",
        "# from urllib.parse import urlparse\n",
        "# from urllib.parse import  urljoin\n",
        "# parentLink= 'https://www.bundesarchiv.de//cocoon/barch/0000/k/0.html'\n",
        "\n",
        "# recheckObj = getSoupObj(url)\n",
        "# leftMenu = recheckObj.select_one('.navimenu_list')\n",
        "# for navLink in leftMenu.findAll('a'):\n",
        "#   nextLink=navLink.get('href')\n",
        "#   b = urljoin(parentLink,nextLink)\n",
        "#   print(nextLink + \" \" + str(b))\n",
        "# # navimenu_list "
      ],
      "metadata": {
        "id": "yb-fCy33mxm2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch the data from web pages via Beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def getSoupObj(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      print(\"getSoupObj\",url)\n",
        "      page = urlopen(url,timeout=webPageReadTimeout)\n",
        "      html = page.read().decode(\"utf-8\")\n",
        "      #Use Beautiful Soup to process the data\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "      # pagetext =soup.get_text()\n",
        "      return soup\n",
        "  except:\n",
        "    print(\"Problem crawling url: \"+url)\n"
      ],
      "metadata": {
        "id": "GKb-XVOIgZix"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limit the Urls we crawl, preventing entrie website to be crawled** <br/>\n",
        "Considering only the archive sub-sites"
      ],
      "metadata": {
        "id": "omhlNDX-iSrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ignoredLinks = ['https://www.bundesarchiv.de/cocoon/barch/']\n",
        "\n",
        "#This Determines if we should allow this and its subset of urls to be crawled\n",
        "def isListPartOfIgnoreLinks(urlToCheck):\n",
        "  resp = False\n",
        "  if urlToCheck != None:\n",
        "    for item in ignoredLinks:\n",
        "      if item in urlToCheck :\n",
        "        resp =True\n",
        "\n",
        "  print('isListPartOfIgnoreLinks()-' + str(urlToCheck)+ \"Allowed ?: \"+ str(resp))\n",
        "  return resp\n"
      ],
      "metadata": {
        "id": "AMCNaKJAiR4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QueueUrlFound(urlVal):\n",
        "  queue_service.put_message(queue_name,urlVal)"
      ],
      "metadata": {
        "id": "j_KjTmLl6Sp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling Logic\n",
        "\n",
        "**Crawl** and find all links for a given Parent Link- Crawls and finds all child links in recursive loop"
      ],
      "metadata": {
        "id": "Swl-lRf-uuns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #use sets instead of list to avoid duplicates, adding default root url so that python considers as a Set\n",
        " #Stopped using set as it can't perform a contains operation to compare and check a string\n",
        "links = [url]\n",
        "def crawlLinks(url):\n",
        "  if url != None:\n",
        "    soup = getSoupObj(url)\n",
        "    if soup != None:\n",
        "      for link in soup.findAll('a'):\n",
        "        nextLink=link.get('href')\n",
        "        if nextLink != None:\n",
        "          # print(\"CrawlLinks - nextink\"+nextLink)\n",
        "          linkToAdd= GetValidURL(nextLink) #Get Processed valid url incase of sub links \n",
        "          if linkToAdd != None and linkToAdd not in links and isListPartOfIgnoreLinks(linkToAdd):\n",
        "            print(\"CrawlLinks - linkToAdd \"+ linkToAdd)\n",
        "            QueueUrlFound(linkToAdd)\n",
        "            links.append(linkToAdd) #Crawl further/ add to set only if valid\n",
        "            crawlLinks(linkToAdd)\n"
      ],
      "metadata": {
        "id": "aWD6BdANHyjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing \n",
        "# newLink= url\n",
        "# ctr = 0\n",
        "# #newLink= \"http://www.bundesarchiv.de/bestaende_findmittel/bestaendeuebersicht/index_frameset.html\" \n",
        "# soup = getSoupObj(newLink)\n",
        "# for link in soup.findAll('a'):\n",
        "#   ctr+=1\n",
        "#   nextLink=link.get('href')\n",
        "#   # print(ctr, nextLink)\n",
        "#   if nextLink != None and IsAbsoluteUrl(nextLink) == False and nextLink[0:1] != \"#\":    \n",
        "#     parsedUrl = nextLink\n",
        "#     if IsAbsoluteUrl(nextLink) == False: #If sublink\n",
        "#       print(ctr, \"Its a Sub Url\",nextLink)\n",
        "#       parsedUrl = JoinUrl(nextLink) #Join with base url and get full path\n",
        "#     print(ctr, parsedUrl)    \n",
        "#   elif IsValidUrl_Regex(nextLink) and nextLink != None: #Only if the link is valid\n",
        "#     print(ctr, \"Valid Url\",nextLink)    \n",
        "  # else: \n",
        "    #print(ctr, \"Not valid based on regex\",nextLink)    "
      ],
      "metadata": {
        "id": "w5JAFjh-kGK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# datetime object containing current date and time\n",
        "print(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "y5Qi0NIs9M5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initiation point to crawl and capture all the data from the website**"
      ],
      "metadata": {
        "id": "HXdr7-5pZc4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crawlLinks(url)"
      ],
      "metadata": {
        "id": "a62Rx4oXu4SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datetime object containing current date and time\n",
        "print(\"End of crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "T865Ialo9wo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a scope of improvement to implementing multi processing for crawling"
      ],
      "metadata": {
        "id": "N_tKzDmzVvJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying this with multi processing approach to run faster\n",
        "# import multiprocessing  \n",
        "\n",
        "# process = multiprocessing.Process(target= crawlLinks, args=(url, ))  \n",
        "# print(\"The number of CPU currently working in system : \", multiprocessing.cpu_count())  \n",
        "# process.start()  \n",
        "# process.join()  \n",
        "\n",
        "# links"
      ],
      "metadata": {
        "id": "NKXJ4NkpjiIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjGuoo2V4sE1",
        "outputId": "5d3be2d4-5d9e-4ff2-81d2-0172edfdf697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4036"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing another pass \n",
        "from urllib.parse import urlparse\n",
        "from urllib.parse import  urljoin\n",
        "\n",
        "parentLink= 'https://www.bundesarchiv.de//cocoon/barch/0000/k/0.html'\n",
        "\n",
        "recheckObj = getSoupObj(url)\n",
        "leftMenu = recheckObj.select_one('.navimenu_list')\n",
        "for navLink in leftMenu.findAll('a'):\n",
        "  nextLink=navLink.get('href')\n",
        "  joinedUrl = urljoin(parentLink,nextLink)\n",
        "  print(nextLink + \" \" + str(joinedUrl))\n",
        "  if links.index(joinedUrl) < 0:\n",
        "    QueueUrlFound(joinedUrl) #Queue if not added already\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axBkLkZIzBcm",
        "outputId": "b42478b0-1463-4622-8723-2d7606692f61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getSoupObj https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\n",
            "#Start https://www.bundesarchiv.de//cocoon/barch/0000/k/0.html#Start\n",
            "../k/k1949k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1949k/index.html\n",
            "../k/k1950k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1950k/index.html\n",
            "../k/k1951k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1951k/index.html\n",
            "../k/k1952k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1952k/index.html\n",
            "../k/k1953k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1953k/index.html\n",
            "../k/k1954k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1954k/index.html\n",
            "../k/k1955k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1955k/index.html\n",
            "../k/k1956k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1956k/index.html\n",
            "../k/k1957k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1957k/index.html\n",
            "../k/k1958k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1958k/index.html\n",
            "../k/k1959k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html\n",
            "../k/k1960k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1960k/index.html\n",
            "../k/k1961k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1961k/index.html\n",
            "../k/k1962k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1962k/index.html\n",
            "../k/k1963k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1963k/index.html\n",
            "../k/k1964k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1964k/index.html\n",
            "../k/k1965k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1965k/index.html\n",
            "../k/k1966k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1966k/index.html\n",
            "../k/k1967k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1967k/index.html\n",
            "../k/k1968k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1968k/index.html\n",
            "../k/k1969k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1969k/index.html\n",
            "../k/k1970k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1970k/index.html\n",
            "../k/k1971k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1971k/index.html\n",
            "../k/k1972k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1972k/index.html\n",
            "../k/k1973k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1973k/index.html\n",
            "../k/k1974k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1974k/index.html\n",
            "../k/k1975k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1975k/index.html\n",
            "../k/k1976k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1976k/index.html\n",
            "../k/k1977k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1977k/index.html\n",
            "../k/k1978k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1978k/index.html\n",
            "../k/k1979k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1979k/index.html\n",
            "../k/k1980k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1980k/index.html\n",
            "../k/k1981k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1981k/index.html\n",
            "../k/k1982k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1982k/index.html\n",
            "../k/k1983k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1983k/index.html\n",
            "../k/k1984k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1984k/index.html\n",
            "../k/k1985k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1985k/index.html\n",
            "../k/k1986k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1986k/index.html\n",
            "../k/k1987k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1987k/index.html\n",
            "../k/k1988k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1988k/index.html\n",
            "../k/k1989k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1989k/index.html\n",
            "../k/k1990k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1990k/index.html\n",
            "../k/k1991k/index.html https://www.bundesarchiv.de/cocoon/barch/0000/k/k1991k/index.html\n",
            "../x/index.html https://www.bundesarchiv.de/cocoon/barch/0000/x/index.html\n",
            "../z/index.html https://www.bundesarchiv.de/cocoon/barch/0000/z/index.html\n",
            "../netzeditionsgrundsaetze.html https://www.bundesarchiv.de/cocoon/barch/0000/netzeditionsgrundsaetze.html\n",
            "../geschaeftsordnung.html https://www.bundesarchiv.de/cocoon/barch/0000/geschaeftsordnung.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JoinUrl(\"../k/k1949k/index.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "eNF5s5tX9ZVl",
        "outputId": "678487a7-fdc9-4a5d-8025-9461b2afede7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseURL https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\n",
            "SubUrl ../k/k1949k/index.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.bundesarchiv.de/cocoon/barch/0000/k/k1949k/index.html'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# links.index('https://www.bundesarchiv.de/cocoon/barch/0000/k/k1949k/index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "Rf4Veywx9ptd",
        "outputId": "5e88fb49-4ec8-4e5f-b716-6f423e819d5b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-afac2537acd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.bundesarchiv.de/cocoon/barch/0000/k/k1949k/index.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'links' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(leftMenu.findAll('a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ISwHjh08mx",
        "outputId": "33b7516a-dca8-4989-e94b-d729a0e0b21b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "876jO5zoz2bJ"
      }
    }
  ]
}