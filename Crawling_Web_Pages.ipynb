{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/Crawling_Web_Pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2mT0Grn274_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IptUs657qgvZ"
      },
      "source": [
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzf52m3bXEnE"
      },
      "source": [
        "Dissertation project for final year\n",
        "\n",
        "\n",
        "---\n",
        "This File Takes care of crawling the given web page for sub urls and pushes those urls to a queue for further steps.\n",
        "\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        "\n",
        "\n",
        "*   Beautiful soup\n",
        "*   Regex\n",
        "*   UrlLib\n",
        "*   Regex\n",
        "\n",
        "Additionally used sys, subprocess, pkg_resources for package installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Ef2pqTXKBS"
      },
      "source": [
        "Install all the  Packages used in the project if missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "5gzsMcj3W4s0"
      },
      "outputs": [],
      "source": [
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QXfJWgD5uTB"
      },
      "outputs": [],
      "source": [
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSnsRFicQbHb"
      },
      "source": [
        "Initialize google drive for storing the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcqiGtcjQarU",
        "outputId": "9f07f923-56b3-4320-c647-1ba5bded723a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Vt-QjFqkVB"
      },
      "source": [
        "**Setup to read the web pages and parse them for NLP Operations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIrH-fCOtmF7"
      },
      "source": [
        "Enter the Root Url -allow the url to be crawled to find other url and finally get all the contents from all links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "lr90vHZUDo3d"
      },
      "outputs": [],
      "source": [
        "#Get the Main Link\n",
        "url = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "ignoredLinks = ['https://www.bundesarchiv.de/cocoon/barch/'] #Limiting the sub url crawl with 1 more more root url\n",
        "outputFolder='/content/gdrive/My Drive/DissertationFiles/'\n",
        "webPageReadTimeout = 10 #For soup object\n",
        "\n",
        "import \n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT46lCUO6IPM"
      },
      "source": [
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "xCAZ2eoh6HMd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "queue_name = \"queue-crawledarchiveurls\"\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7aEJLxzeqRt"
      },
      "source": [
        "Get the Joined correct url for the given sub url, by deauflt the base url is the Starting page to crawl - which could be overridden if required. <br/>\n",
        "\n",
        "JoinUrl()  - Joins Sub url with parent url <br/>\n",
        "IsAbolsuteUrl()  - Returns False in case of  Sub url  <br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sinAUbMcU1i",
        "outputId": "48e219be-4731-4d2f-e518-8d1897fd8dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IsAbsoluteUrl False\n",
            "BaseURL https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\n",
            "SubUrl ../k/k1983k/index.html\n",
            "Joined Url https://www.bundesarchiv.de/cocoon/barch/0000/k/k1983k/index.html\n",
            "IsAbsoluteUrl False\n",
            "BaseURL https://www.bundesarchiv.de/\n",
            "SubUrl ../k/k1983k/index.html\n",
            "Joined Url https://www.bundesarchiv.de/k/k1983k/index.html\n",
            "IsAbsoluteUrl True\n",
            "IsAbsoluteUrl False\n",
            "IsAbsoluteUrl True\n"
          ]
        }
      ],
      "source": [
        "from urllib.parse import urljoin\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def JoinUrl(subUrl, baseUrl = url):\n",
        "  print(\"BaseURL\",baseUrl)\n",
        "  print(\"SubUrl\",subUrl)\n",
        "  return urljoin(baseUrl,subUrl)\n",
        "\n",
        "\n",
        "def IsAbsoluteUrl(url):\n",
        "    return bool(urlparse(url).netloc)\n",
        "\n",
        "\n",
        "#Testing Scenario 1\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "#Testing Scenario 2\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de/\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://google.com\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klLXcIYJUyMj"
      },
      "source": [
        "**Function which returns if it is a valid url using Regex**\n",
        "\n",
        "Source code from \n",
        "[Stack overflow](https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7G__bpzU2ED",
        "outputId": "b16107e0-8ed2-4a5f-db1a-7c4277c5a8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://www.google.com True\n",
            "https://www.google.com True\n",
            "http://google.com True\n",
            "https://google.com True\n",
            "www.google.com True\n",
            "google.com True\n",
            "http://www.google.com/~as_db3.2123/134-1a True\n",
            "https://www.google.com/~as_db3.2123/134-1a True\n",
            "http://google.com/~as_db3.2123/134-1a True\n",
            "https://google.com/~as_db3.2123/134-1a True\n",
            "www.google.com/~as_db3.2123/134-1a True\n",
            "google.com/~as_db3.2123/134-1a True\n",
            "http://www.google.co.uk True\n",
            "https://www.google.co.uk True\n",
            "http://google.co.uk True\n",
            "https://google.co.uk True\n",
            "www.google.co.uk True\n",
            "google.co.uk True\n",
            "http://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "https://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "http://google.co.uk/~as_db3.2123/134-1a True\n",
            "https://google.co.uk/~as_db3.2123/134-1a True\n",
            "www.google.co.uk/~as_db3.2123/134-1a True\n",
            "google.co.uk/~as_db3.2123/134-1a True\n",
            "https://... False\n",
            "https://.. False\n",
            "https://. False\n",
            "https://.google.com False\n",
            "https://..google.com False\n",
            "https://...google.com False\n",
            "https://.google..com False\n",
            "https://.google...comhttps://...google..com False\n",
            "https://...google...com False\n",
            ".google.com False\n",
            ".google.co.https://google.co. False\n"
          ]
        }
      ],
      "source": [
        "#Create regex\n",
        "import re\n",
        "\n",
        "regex = re.compile(\n",
        "    r\"(\\w+://)?\"                # protocol                      (optional)\n",
        "    r\"(\\w+\\.)?\"                 # host                          (optional)\n",
        "    r\"((\\w+)\\.(\\w+))\"           # domain\n",
        "    r\"(\\.\\w+)*\"                 # top-level domain              (optional, can have > 1)\n",
        "    r\"([\\w\\-\\._\\~/]*)*(?<!\\.)\"  # path, params, anchors, etc.   (optional)\n",
        ")\n",
        "\n",
        "def IsValidUrl_Regex(url):\n",
        "  try: \n",
        "    return regex.match(url).span()[1] - regex.match(url).span()[0] == len(url)\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "cases = [\n",
        "    \"http://www.google.com\",\n",
        "    \"https://www.google.com\",\n",
        "    \"http://google.com\",\n",
        "    \"https://google.com\",\n",
        "    \"www.google.com\",\n",
        "    \"google.com\",\n",
        "    \"http://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"http://google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://google.com/~as_db3.2123/134-1a\",\n",
        "    \"www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"google.com/~as_db3.2123/134-1a\",\n",
        "    # .co.uk top level\n",
        "    \"http://www.google.co.uk\",\n",
        "    \"https://www.google.co.uk\",\n",
        "    \"http://google.co.uk\",\n",
        "    \"https://google.co.uk\",\n",
        "    \"www.google.co.uk\",\n",
        "    \"google.co.uk\",\n",
        "    \"http://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"http://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://...\",\n",
        "    \"https://..\",\n",
        "    \"https://.\",\n",
        "    \"https://.google.com\",\n",
        "    \"https://..google.com\",\n",
        "    \"https://...google.com\",\n",
        "    \"https://.google..com\",\n",
        "    \"https://.google...com\"\n",
        "    \"https://...google..com\",\n",
        "    \"https://...google...com\",\n",
        "    \".google.com\",\n",
        "    \".google.co.\"\n",
        "    \"https://google.co.\"\n",
        "]\n",
        "\n",
        "for c in cases:\n",
        "  print(c, IsValidUrl_Regex(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4AVWIeIYn2X"
      },
      "source": [
        "Recursive function to crawl and find all the valid Urls, for further processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "yb-fCy33mxm2"
      },
      "outputs": [],
      "source": [
        "def GetValidURL(urlval):\n",
        "  # print(\"GetValidURL\",urlval)\n",
        "  linkToAdd= None\n",
        "  if urlval != None and IsAbsoluteUrl(urlval) == False and urlval[0:1] != \"#\":    \n",
        "    # print(\"GetValidURL-sub url parent\",urlval)\n",
        "    linkToAdd = urlval\n",
        "    if IsAbsoluteUrl(urlval) == False: #If sublink\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -false\",urlval)\n",
        "      linkToAdd = JoinUrl(urlval) #Join with base url and get full path \n",
        "    else:\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -true\",urlval)\n",
        "      linkToAdd = urlval       \n",
        "  elif IsValidUrl_Regex(urlval) and urlval != None: #Only if the link is valid\n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -valid link\",urlval)\n",
        "    linkToAdd = urlval\n",
        "  else: \n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -Not valid link\",urlval)\n",
        "    linkToAdd= None\n",
        "    #print(ctr, \"Not valid based on regex\",nextLink) - Dont do anything, Ignore invalid Urls\n",
        "\n",
        "  return linkToAdd\n",
        "\n",
        "\n",
        " \n",
        "# from urllib.parse import urlparse\n",
        "# from urllib.parse import  urljoin\n",
        "# parentLink= 'https://www.bundesarchiv.de//cocoon/barch/0000/k/0.html'\n",
        "\n",
        "# recheckObj = getSoupObj(url)\n",
        "# leftMenu = recheckObj.select_one('.navimenu_list')\n",
        "# for navLink in leftMenu.findAll('a'):\n",
        "#   nextLink=navLink.get('href')\n",
        "#   b = urljoin(parentLink,nextLink)\n",
        "#   print(nextLink + \" \" + str(b))\n",
        "# # navimenu_list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "GKb-XVOIgZix"
      },
      "outputs": [],
      "source": [
        "#Fetch the data from web pages via Beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def getSoupObj(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      print(\"getSoupObj\",url)\n",
        "      page = urlopen(url,timeout=webPageReadTimeout)\n",
        "      html = page.read().decode(\"utf-8\")\n",
        "      #Use Beautiful Soup to process the data\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "      # pagetext =soup.get_text()\n",
        "      return soup\n",
        "  except:\n",
        "    print(\"Problem crawling url: \"+url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omhlNDX-iSrn"
      },
      "source": [
        "**Limit the Urls we crawl, preventing entrie website to be crawled** <br/>\n",
        "Considering only the archive sub-sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "AMCNaKJAiR4J"
      },
      "outputs": [],
      "source": [
        "# ignoredLinks = ['https://www.bundesarchiv.de/cocoon/barch/']\n",
        "\n",
        "#This Determines if we should allow this and its subset of urls to be crawled\n",
        "def isListPartOfIgnoreLinks(urlToCheck):\n",
        "  resp = False\n",
        "  if urlToCheck != None:\n",
        "    for item in ignoredLinks:\n",
        "      if item in urlToCheck :\n",
        "        resp =True\n",
        "\n",
        "  print('isListPartOfIgnoreLinks()-' + str(urlToCheck)+ \"Allowed ?: \"+ str(resp))\n",
        "  return resp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "j_KjTmLl6Sp2"
      },
      "outputs": [],
      "source": [
        "def QueueUrlFound(urlVal):\n",
        "  queue_service.put_message(queue_name,urlVal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "emNAnNhoyd_x"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def check_url_exists(chkurl: str):\n",
        "  try:\n",
        "    response = requests.get(chkurl)\n",
        "    if response.status_code == 200:\n",
        "        print('Web site exists')\n",
        "        return True\n",
        "    else:\n",
        "        print('Web site does not exist') \n",
        "  except:\n",
        "    print('Problem processing url')    \n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swl-lRf-uuns"
      },
      "source": [
        "# Crawling Logic\n",
        "\n",
        "**Crawl** and find all links for a given Parent Link- Crawls and finds all child links in recursive loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "aWD6BdANHyjr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#use sets instead of list to avoid duplicates, adding default root url so that python considers as a Set\n",
        "#Stopped using set as it can't perform a contains operation to compare and check a string\n",
        "#links = [url]\n",
        "\n",
        "import pandas as pd\n",
        "arrHeader = ['Url','ParentUrl','NextLink','linkToAdd','IsvalidUrl']\n",
        "# urlDetails = pd.DataFrame([[url, '','', '', True]],  columns= arrHeader)\n",
        "arrdettt= []\n",
        "\n",
        "def crawlLinks(url,parentUrl):\n",
        "  if url != None:\n",
        "    soup = getSoupObj(url)\n",
        "    if soup != None:\n",
        "      for link in soup.findAll('a'):\n",
        "        nextLink=link.get('href')\n",
        "        if nextLink != None:\n",
        "          # print(\"CrawlLinks - nextink\"+nextLink)\n",
        "          linkToAdd= GetValidURL(nextLink) #Get Processed valid url incase of sub links \n",
        "          if linkToAdd != None and url in urlDetails['Url'].values and isListPartOfIgnoreLinks(linkToAdd) and check_url_exists(linkToAdd):\n",
        "            #urlDetails.append(pd.DataFrame([[url, parentUrl,nextLink, linkToAdd,True]],  columns=arrHeader))\n",
        "            arrdettt.append([url, parentUrl,nextLink, linkToAdd,True])\n",
        "            print(\"CrawlLinks - linkToAdd \"+ linkToAdd)\n",
        "            # QueueUrlFound(linkToAdd)\n",
        "            # links.append(linkToAdd) #Crawl further/ add to set only if valid\n",
        "            crawlLinks(linkToAdd,url )\n",
        "          else:\n",
        "            # urlDetails.append(pd.DataFrame([[url, parentUrl,nextLink, linkToAdd,False]] ,columns=arrHeader))\n",
        "            arrdettt.append([url, parentUrl,nextLink, linkToAdd,False])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "w5JAFjh-kGK3"
      },
      "outputs": [],
      "source": [
        "#Testing \n",
        "# newLink= url\n",
        "# ctr = 0\n",
        "# #newLink= \"http://www.bundesarchiv.de/bestaende_findmittel/bestaendeuebersicht/index_frameset.html\" \n",
        "# soup = getSoupObj(newLink)\n",
        "# for link in soup.findAll('a'):\n",
        "#   ctr+=1\n",
        "#   nextLink=link.get('href')\n",
        "#   # print(ctr, nextLink)\n",
        "#   if nextLink != None and IsAbsoluteUrl(nextLink) == False and nextLink[0:1] != \"#\":    \n",
        "#     parsedUrl = nextLink\n",
        "#     if IsAbsoluteUrl(nextLink) == False: #If sublink\n",
        "#       print(ctr, \"Its a Sub Url\",nextLink)\n",
        "#       parsedUrl = JoinUrl(nextLink) #Join with base url and get full path\n",
        "#     print(ctr, parsedUrl)    \n",
        "#   elif IsValidUrl_Regex(nextLink) and nextLink != None: #Only if the link is valid\n",
        "#     print(ctr, \"Valid Url\",nextLink)    \n",
        "  # else: \n",
        "    #print(ctr, \"Not valid based on regex\",nextLink)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Qi0NIs9M5A",
        "outputId": "67a06631-728d-4499-8e71-58479e5115d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initiaing crawling: 25/04/2022 06:43:09\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "# datetime object containing current date and time\n",
        "print(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXdr7-5pZc4s"
      },
      "source": [
        "**Initiation point to crawl and capture all the data from the website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "a62Rx4oXu4SM"
      },
      "outputs": [],
      "source": [
        "# crawlLinks(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKXJ4NkpjiIk"
      },
      "outputs": [],
      "source": [
        "#Trying this with multi processing approach to run faster\n",
        "import multiprocessing  \n",
        "parurl= url\n",
        "process = multiprocessing.Process(target= crawlLinks, args=(url,parurl, ))  \n",
        "print(\"The number of CPU currently working in system : \", multiprocessing.cpu_count())  \n",
        "process.start()  \n",
        "process.join()  \n",
        "\n",
        "# links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T865Ialo9wo_",
        "outputId": "705b203a-d5c1-4928-9c14-9b55845ef8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of crawling: 25/04/2022 12:17:32\n"
          ]
        }
      ],
      "source": [
        "# datetime object containing current date and time\n",
        "print(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(arrdettt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLvdS9ledDqF",
        "outputId": "8c0a054c-8f18-4b9f-9df8-53b609fa586b"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_tKzDmzVvJB"
      },
      "source": [
        "Creating a scope of improvement to implementing multi processing for crawling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axBkLkZIzBcm"
      },
      "outputs": [],
      "source": [
        "#Performing another pass \n",
        "from urllib.parse import urlparse\n",
        "from urllib.parse import  urljoin\n",
        "\n",
        "#parentLink= 'https://www.bundesarchiv.de//cocoon/barch/0000/k/0.html'\n",
        "parentLink = url\n",
        "\n",
        "recheckObj = getSoupObj(url)\n",
        "leftMenu = recheckObj.select_one('.navimenu_list')\n",
        "for navLink in leftMenu.findAll('a'):\n",
        "  nextLink=navLink.get('href')\n",
        "  linkToAdd = urljoin(parentLink,nextLink)\n",
        "  print('Link to add '+ linkToAdd)\n",
        "  #if linkToAdd != None and linkToAdd not in urlDetails['linkToAdd'].values and isListPartOfIgnoreLinks(linkToAdd) and check_url_exists(linkToAdd):\n",
        "  if linkToAdd != None and isListPartOfIgnoreLinks(linkToAdd):\n",
        "    # print([parentLink, parentLink,nextLink, linkToAdd,True])\n",
        "    print(parentLink, url,nextLink, linkToAdd,True)\n",
        "    #urlDetails.append(pd.DataFrame([[url, parentLink,nextLink, linkToAdd,check_url_exists(linkToAdd)]], columns=arrHeader))\n",
        "    arrdettt.append([url, parentLink,nextLink, linkToAdd,check_url_exists(linkToAdd)])\n",
        "  \n",
        "  \n",
        "\n",
        "#  urlDetails.append(pd.DataFrame([[url, parentUrl,nextLink, linkToAdd,True]],  columns=arrHeader))\n",
        "# QueueUrlFound(joinedUrl) #Queue if not added already\n",
        "\n",
        "urlDetails= pd.DataFrame(arrdettt, columns=arrHeader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1sXNr0TCjgU"
      },
      "outputs": [],
      "source": [
        "urlDetails.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jBO1p0JQ5Wq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "csvWritePath = outputFolder+'capturedUrl.csv'+ datetime.now(IST).strftime(\"%m_%d_%Y__%H_%M_%S\")+'.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4DCP344Q6pK"
      },
      "outputs": [],
      "source": [
        "#Write Dataframe to CSV file\n",
        "urlDetails.to_csv(csvWritePath, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876jO5zoz2bJ"
      },
      "source": [
        "We have crawled and found all the possible web pages and persisted the output to a csv file"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Crawling Web Pages.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP77/W4CmkIbiUK7zO9DGCD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}