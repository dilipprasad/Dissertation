{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Limited_WebCrawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkjFv+yqafmsJbTMqhnFps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/Limited_WebCrawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**\n",
        "\n",
        "Dissertation project for final year\n",
        "\n",
        "\n",
        "---\n",
        "This File Takes care of crawling the given web page for sub urls and pushes those urls to a queue for further steps.\n",
        "\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        "\n",
        "\n",
        "*   Beautiful soup\n",
        "*   Regex\n",
        "*   UrlLib\n",
        "*   Regex\n",
        "\n",
        "Additionally used sys, subprocess, pkg_resources for package installation\n",
        "\n",
        "Install all the  Packages used in the project if missing\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilhlJIxyNRKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0RC7-NJNQdk",
        "outputId": "c1d82835-8ddb-47ae-d53f-a7abf744c226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure in /usr/local/lib/python3.7/dist-packages (4.0.0)\n",
            "Requirement already satisfied: azure-applicationinsights~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure) (0.1.1)\n",
            "Requirement already satisfied: azure-servicebus~=0.21.1 in /usr/local/lib/python3.7/dist-packages (from azure) (0.21.1)\n",
            "Requirement already satisfied: azure-servicefabric~=6.3.0.0 in /usr/local/lib/python3.7/dist-packages (from azure) (6.3.0.0)\n",
            "Requirement already satisfied: azure-cosmosdb-table~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure) (1.0.6)\n",
            "Requirement already satisfied: azure-graphrbac~=0.40.0 in /usr/local/lib/python3.7/dist-packages (from azure) (0.40.0)\n",
            "Requirement already satisfied: azure-batch~=4.1 in /usr/local/lib/python3.7/dist-packages (from azure) (4.1.3)\n",
            "Requirement already satisfied: azure-storage-blob~=1.3 in /usr/local/lib/python3.7/dist-packages (from azure) (1.5.0)\n",
            "Requirement already satisfied: azure-eventgrid~=1.1 in /usr/local/lib/python3.7/dist-packages (from azure) (1.3.0)\n",
            "Requirement already satisfied: azure-datalake-store~=0.0.18 in /usr/local/lib/python3.7/dist-packages (from azure) (0.0.52)\n",
            "Requirement already satisfied: azure-mgmt~=4.0 in /usr/local/lib/python3.7/dist-packages (from azure) (4.0.0)\n",
            "Requirement already satisfied: azure-storage-queue~=1.3 in /usr/local/lib/python3.7/dist-packages (from azure) (1.4.0)\n",
            "Requirement already satisfied: azure-storage-file~=1.3 in /usr/local/lib/python3.7/dist-packages (from azure) (1.4.0)\n",
            "Requirement already satisfied: azure-loganalytics~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure) (0.1.1)\n",
            "Requirement already satisfied: azure-servicemanagement-legacy~=0.20.6 in /usr/local/lib/python3.7/dist-packages (from azure) (0.20.7)\n",
            "Requirement already satisfied: azure-keyvault~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure) (1.1.0)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.7/dist-packages (from azure-applicationinsights~=0.1.0->azure) (0.6.21)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.7/dist-packages (from azure-applicationinsights~=0.1.0->azure) (1.1.28)\n",
            "Requirement already satisfied: azure-nspkg>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from azure-batch~=4.1->azure) (3.0.2)\n",
            "Requirement already satisfied: msrestazure<2.0.0,>=0.4.20 in /usr/local/lib/python3.7/dist-packages (from azure-batch~=4.1->azure) (0.6.4)\n",
            "Requirement already satisfied: azure-cosmosdb-nspkg>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from azure-cosmosdb-table~=1.0->azure) (2.0.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from azure-cosmosdb-table~=1.0->azure) (37.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from azure-cosmosdb-table~=1.0->azure) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from azure-cosmosdb-table~=1.0->azure) (2.8.2)\n",
            "Requirement already satisfied: adal>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from azure-datalake-store~=0.0.18->azure) (1.2.7)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from azure-datalake-store~=0.0.18->azure) (1.15.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from adal>=0.4.2->azure-datalake-store~=0.0.18->azure) (2.3.0)\n",
            "Requirement already satisfied: azure-mgmt-relay~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-marketplaceordering~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-servicebus~=0.5.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.5.3)\n",
            "Requirement already satisfied: azure-mgmt-batch~=5.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (5.0.1)\n",
            "Requirement already satisfied: azure-mgmt-advisor~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.0.1)\n",
            "Requirement already satisfied: azure-mgmt-managementpartner~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.1)\n",
            "Requirement already satisfied: azure-mgmt-containerregistry~=2.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.8.0)\n",
            "Requirement already satisfied: azure-mgmt-monitor~=0.5.2 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.5.2)\n",
            "Requirement already satisfied: azure-mgmt-recoveryservices~=0.3.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.3.0)\n",
            "Requirement already satisfied: azure-mgmt-reservations~=0.2.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.1)\n",
            "Requirement already satisfied: azure-mgmt-signalr~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.1)\n",
            "Requirement already satisfied: azure-mgmt-sql~=0.9.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.9.1)\n",
            "Requirement already satisfied: azure-mgmt-logic~=3.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (3.0.0)\n",
            "Requirement already satisfied: azure-mgmt-cosmosdb~=0.4.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.4.1)\n",
            "Requirement already satisfied: azure-mgmt-billing~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-resource~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.2.0)\n",
            "Requirement already satisfied: azure-mgmt-trafficmanager~=0.50.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.50.0)\n",
            "Requirement already satisfied: azure-mgmt-datalake-store~=0.5.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.5.0)\n",
            "Requirement already satisfied: azure-mgmt-keyvault~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.1.0)\n",
            "Requirement already satisfied: azure-mgmt-eventgrid~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.0.0)\n",
            "Requirement already satisfied: azure-mgmt-policyinsights~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-consumption~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.0.0)\n",
            "Requirement already satisfied: azure-mgmt-web~=0.35.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.35.0)\n",
            "Requirement already satisfied: azure-mgmt-iothub~=0.5.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.5.0)\n",
            "Requirement already satisfied: azure-mgmt-devtestlabs~=2.2 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.2.0)\n",
            "Requirement already satisfied: azure-mgmt-iothubprovisioningservices~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-notificationhubs~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.1.0)\n",
            "Requirement already satisfied: azure-mgmt-hanaonazure~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.1)\n",
            "Requirement already satisfied: azure-mgmt-loganalytics~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-iotcentral~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-devspaces~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-batchai~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.0.0)\n",
            "Requirement already satisfied: azure-mgmt-machinelearningcompute~=0.4.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.4.1)\n",
            "Requirement already satisfied: azure-mgmt-containerservice~=4.2 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (4.4.0)\n",
            "Requirement already satisfied: azure-mgmt-rdbms~=1.2 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.9.0)\n",
            "Requirement already satisfied: azure-mgmt-subscription~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-dns~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.1.0)\n",
            "Requirement already satisfied: azure-mgmt-datamigration~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.0.0)\n",
            "Requirement already satisfied: azure-mgmt-datalake-analytics~=0.6.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.6.0)\n",
            "Requirement already satisfied: azure-mgmt-eventhub~=2.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.6.0)\n",
            "Requirement already satisfied: azure-mgmt-storage~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.0.0)\n",
            "Requirement already satisfied: azure-mgmt-datafactory~=0.6.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.6.0)\n",
            "Requirement already satisfied: azure-mgmt-applicationinsights~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.1)\n",
            "Requirement already satisfied: azure-mgmt-compute~=4.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (4.6.2)\n",
            "Requirement already satisfied: azure-mgmt-maps~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-cdn~=3.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (3.1.0)\n",
            "Requirement already satisfied: azure-mgmt-media~=1.0.0rc2 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.0.0)\n",
            "Requirement already satisfied: azure-mgmt-recoveryservicesbackup~=0.3.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.3.0)\n",
            "Requirement already satisfied: azure-mgmt-managementgroups~=0.1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.1.0)\n",
            "Requirement already satisfied: azure-mgmt-search~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.1.0)\n",
            "Requirement already satisfied: azure-mgmt-containerinstance~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.5.0)\n",
            "Requirement already satisfied: azure-mgmt-network~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.7.0)\n",
            "Requirement already satisfied: azure-mgmt-redis~=5.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (5.0.0)\n",
            "Requirement already satisfied: azure-mgmt-authorization~=0.50.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.50.0)\n",
            "Requirement already satisfied: azure-mgmt-cognitiveservices~=3.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (3.0.0)\n",
            "Requirement already satisfied: azure-mgmt-scheduler~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.0.0)\n",
            "Requirement already satisfied: azure-mgmt-servicefabric~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-msi~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (0.2.0)\n",
            "Requirement already satisfied: azure-mgmt-powerbiembedded~=2.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (2.0.0)\n",
            "Requirement already satisfied: azure-mgmt-commerce~=1.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt~=4.0->azure) (1.0.1)\n",
            "Requirement already satisfied: azure-mgmt-nspkg>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt-advisor~=1.0->azure-mgmt~=4.0->azure) (3.0.2)\n",
            "Requirement already satisfied: azure-mgmt-datalake-nspkg>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from azure-mgmt-datalake-analytics~=0.6.0->azure-mgmt~=4.0->azure) (3.0.1)\n",
            "Requirement already satisfied: azure-storage-common~=1.4 in /usr/local/lib/python3.7/dist-packages (from azure-storage-blob~=1.3->azure) (1.4.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->azure-datalake-store~=0.0.18->azure) (2.21)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-applicationinsights~=0.1.0->azure) (1.3.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-applicationinsights~=0.1.0->azure) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-applicationinsights~=0.1.0->azure) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate>=0.6.0->msrest>=0.6.21->azure-applicationinsights~=0.1.0->azure) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->azure-cosmosdb-table~=1.0->azure) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->azure-cosmosdb-table~=1.0->azure) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->azure-cosmosdb-table~=1.0->azure) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-applicationinsights~=0.1.0->azure) (3.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement azure-storages (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for azure-storages\u001b[0m\n",
            "Requirement already satisfied: azure-storage-queue in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: azure-common>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from azure-storage-queue) (1.1.28)\n",
            "Requirement already satisfied: azure-storage-common~=1.4 in /usr/local/lib/python3.7/dist-packages (from azure-storage-queue) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from azure-storage-common~=1.4->azure-storage-queue) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from azure-storage-common~=1.4->azure-storage-queue) (2.23.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from azure-storage-common~=1.4->azure-storage-queue) (37.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->azure-storage-common~=1.4->azure-storage-queue) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->azure-storage-common~=1.4->azure-storage-queue) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->azure-storage-common~=1.4->azure-storage-queue) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage-common~=1.4->azure-storage-queue) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage-common~=1.4->azure-storage-queue) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage-common~=1.4->azure-storage-queue) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->azure-storage-common~=1.4->azure-storage-queue) (2021.10.8)\n",
            "Requirement already satisfied: azure-data-tables in /usr/local/lib/python3.7/dist-packages (12.3.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from azure-data-tables) (1.23.1)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.7/dist-packages (from azure-data-tables) (0.6.21)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-data-tables) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-data-tables) (4.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-core<2.0.0,>=1.15.0->azure-data-tables) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-data-tables) (1.3.1)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-data-tables) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-data-tables) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-data-tables) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-data-tables) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.15.0->azure-data-tables) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-data-tables) (3.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement urlparse (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for urlparse\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "from traceback import format_exc\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"Initialize google drive for storing the output\""
      ],
      "metadata": {
        "id": "Yt1pnjR-NeW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vOP-Il5rNatQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"**Setup to read the web pages and parse them for NLP Operations**\n",
        "\n",
        "Enter the Root Url -allow the url to be crawled to find other url and finally get all the contents from all links\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "BFoVMBr3Nqe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Main Link\n",
        "url = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "ignoredLinks = ['https://www.bundesarchiv.de/DE/Navigation/'] #Limiting the sub url crawl with 1 more more root url\n",
        "blackListedUrls=  ['https://www.bundesarchiv.de/','http://www.bundesarchiv.de/']#do not crawl these urls\n",
        "allowedDomains = ['www.bundesarchiv.de']  #Allowed List of domains, ignore the rest like twitter, facebook\n",
        "outputFolder='/content/gdrive/My Drive/DissertationFiles/'\n",
        "webPageReadTimeout = 10 #For soup object\n",
        "lnkCnt = 0"
      ],
      "metadata": {
        "id": "Edj5fT3_NrHk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pytz\n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ],
      "metadata": {
        "id": "oN5WcT_rOCcl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n",
        "\n"
      ],
      "metadata": {
        "id": "XJZhHUzeOEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "queue_name = \"queue-crawledarchiveurls\"\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode"
      ],
      "metadata": {
        "id": "FPgTUxLpOH8c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the Joined correct url for the given sub url, by deauflt the base url is the Starting page to crawl - which could be overridden if required. <br/>\n",
        "\n",
        "JoinUrl()  - Joins Sub url with parent url <br/>\n",
        "IsAbolsuteUrl()  - Returns False in case of  Sub url  <br/>"
      ],
      "metadata": {
        "id": "XIyf-eeLONvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from urllib.parse import urljoin\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def JoinUrl(subUrl, baseUrl = url):\n",
        "  # print(\"BaseURL\",baseUrl)\n",
        "  # print(\"SubUrl\",subUrl)\n",
        "  return urljoin(baseUrl,subUrl)\n",
        "\n",
        "\n",
        "def IsAbsoluteUrl(url):\n",
        "    return bool(urlparse(url).netloc)\n"
      ],
      "metadata": {
        "id": "2WwIewwdOLqM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Testing Scenario 1\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "#Testing Scenario 2\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de/\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://google.com\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqevpNcdOE-0",
        "outputId": "3257c06a-fd51-4a67-f21b-6e8fccfab229"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IsAbsoluteUrl False\n",
            "Joined Url https://www.bundesarchiv.de/cocoon/barch/0000/k/k1983k/index.html\n",
            "IsAbsoluteUrl False\n",
            "Joined Url https://www.bundesarchiv.de/k/k1983k/index.html\n",
            "IsAbsoluteUrl True\n",
            "IsAbsoluteUrl False\n",
            "IsAbsoluteUrl True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function which returns if it is a valid url using Regex**\n",
        "\n",
        "Source code from \n",
        "[Stack overflow](https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not)\n"
      ],
      "metadata": {
        "id": "ByOC1MCTOZiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create regex\n",
        "import re\n",
        "\n",
        "regex = re.compile(\n",
        "    r\"(\\w+://)?\"                # protocol                      (optional)\n",
        "    r\"(\\w+\\.)?\"                 # host                          (optional)\n",
        "    r\"((\\w+)\\.(\\w+))\"           # domain\n",
        "    r\"(\\.\\w+)*\"                 # top-level domain              (optional, can have > 1)\n",
        "    r\"([\\w\\-\\._\\~/]*)*(?<!\\.)\"  # path, params, anchors, etc.   (optional)\n",
        ")\n",
        "\n",
        "def IsValidUrl_Regex(url):\n",
        "  try: \n",
        "    return regex.match(url).span()[1] - regex.match(url).span()[0] == len(url)\n",
        "  except:\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "TicreGKOOaIc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "cases = [\n",
        "    \"http://www.google.com\",\n",
        "    \"https://www.google.com\",\n",
        "    \"http://google.com\",\n",
        "    \"https://google.com\",\n",
        "    \"www.google.com\",\n",
        "    \"google.com\",\n",
        "    \"http://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"http://google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://google.com/~as_db3.2123/134-1a\",\n",
        "    \"www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"google.com/~as_db3.2123/134-1a\",\n",
        "    # .co.uk top level\n",
        "    \"http://www.google.co.uk\",\n",
        "    \"https://www.google.co.uk\",\n",
        "    \"http://google.co.uk\",\n",
        "    \"https://google.co.uk\",\n",
        "    \"www.google.co.uk\",\n",
        "    \"google.co.uk\",\n",
        "    \"http://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"http://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://...\",\n",
        "    \"https://..\",\n",
        "    \"https://.\",\n",
        "    \"https://.google.com\",\n",
        "    \"https://..google.com\",\n",
        "    \"https://...google.com\",\n",
        "    \"https://.google..com\",\n",
        "    \"https://.google...com\"\n",
        "    \"https://...google..com\",\n",
        "    \"https://...google...com\",\n",
        "    \".google.com\",\n",
        "    \".google.co.\"\n",
        "    \"https://google.co.\"\n",
        "]\n",
        "\n",
        "for c in cases:\n",
        "  print(c, IsValidUrl_Regex(c))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsXGlxs0OeCW",
        "outputId": "567fba6b-6229-489f-e239-a2f3b24eca60"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://www.google.com True\n",
            "https://www.google.com True\n",
            "http://google.com True\n",
            "https://google.com True\n",
            "www.google.com True\n",
            "google.com True\n",
            "http://www.google.com/~as_db3.2123/134-1a True\n",
            "https://www.google.com/~as_db3.2123/134-1a True\n",
            "http://google.com/~as_db3.2123/134-1a True\n",
            "https://google.com/~as_db3.2123/134-1a True\n",
            "www.google.com/~as_db3.2123/134-1a True\n",
            "google.com/~as_db3.2123/134-1a True\n",
            "http://www.google.co.uk True\n",
            "https://www.google.co.uk True\n",
            "http://google.co.uk True\n",
            "https://google.co.uk True\n",
            "www.google.co.uk True\n",
            "google.co.uk True\n",
            "http://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "https://www.google.co.uk/~as_db3.2123/134-1a True\n",
            "http://google.co.uk/~as_db3.2123/134-1a True\n",
            "https://google.co.uk/~as_db3.2123/134-1a True\n",
            "www.google.co.uk/~as_db3.2123/134-1a True\n",
            "google.co.uk/~as_db3.2123/134-1a True\n",
            "https://... False\n",
            "https://.. False\n",
            "https://. False\n",
            "https://.google.com False\n",
            "https://..google.com False\n",
            "https://...google.com False\n",
            "https://.google..com False\n",
            "https://.google...comhttps://...google..com False\n",
            "https://...google...com False\n",
            ".google.com False\n",
            ".google.co.https://google.co. False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process and find valid url"
      ],
      "metadata": {
        "id": "Qs4vTGkFOkS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def GetValidURL(urlval,ParentUrlVal):\n",
        "  # print(\"GetValidURL\",urlval)\n",
        "  linkToAdd= None\n",
        "  if urlval != None and IsAbsoluteUrl(urlval) == False and urlval[0:1] != \"#\":    \n",
        "    # print(\"GetValidURL-sub url parent\",urlval)\n",
        "    #linkToAdd = urlval\n",
        "    linkToAdd= None\n",
        "    if IsAbsoluteUrl(urlval) == False: #If sublink\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -false\",urlval)\n",
        "      linkToAdd = JoinUrl(urlval,ParentUrlVal) #Join with base url and get full path \n",
        "    else:\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -true\",urlval)\n",
        "      linkToAdd = urlval       \n",
        "  elif IsValidUrl_Regex(urlval) and urlval != None: #Only if the link is valid\n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -valid link\",urlval)\n",
        "    linkToAdd = urlval\n",
        "  else: \n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -Not valid link\",urlval)\n",
        "    linkToAdd= None\n",
        "    #print(ctr, \"Not valid based on regex\",nextLink) - Dont do anything, Ignore invalid Urls\n",
        "\n",
        "  return linkToAdd"
      ],
      "metadata": {
        "id": "T18dg0dyOkyn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Fetch the data from web pages via Beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def getSoupObj(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      page = urlopen(url,timeout=webPageReadTimeout)\n",
        "      html = page.read().decode(\"utf-8\")\n",
        "      print(\"getSoupObj\",url)\n",
        "      #Use Beautiful Soup to process the data\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "      # pagetext =soup.get_text()\n",
        "      return soup\n",
        "  except:\n",
        "    print(\"Problem crawling url: \"+url)\n",
        "  return None"
      ],
      "metadata": {
        "id": "lf2_fcdaOpFS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limit the Urls we crawl, preventing entrie website to be crawled** <br/>\n",
        "Considering only the archive sub-sites"
      ],
      "metadata": {
        "id": "C1zHbk6nOwWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This Determines if we should allow this and its subset of urls to be crawled\n",
        "def isListPartOfIgnoreLinks(urlToCheck):\n",
        "  resp = False\n",
        "  if urlToCheck != None:\n",
        "    for item in ignoredLinks:\n",
        "      if item in urlToCheck :\n",
        "        resp =True\n",
        "        print('isListPartOfIgnoreLinks()-' + str(urlToCheck)+ \"Allowed ?: \"+ str(resp))#print only if ignored\n",
        "  \n",
        "  \n",
        "  return resp\n",
        "\n"
      ],
      "metadata": {
        "id": "m44ARPX4OsU2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def QueueUrlFound(urlVal):\n",
        "  print(\"Queueing url: \"+urlVal)\n",
        "  queue_service.put_message(queue_name,urlVal)\n"
      ],
      "metadata": {
        "id": "OPI8ro-kOyyP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def check_url_exists(chkurl: str):\n",
        "  try:\n",
        "    response = requests.get(chkurl)\n",
        "    if response.status_code == 200:\n",
        "        print('Web site exists')\n",
        "        return True\n",
        "    else:\n",
        "        print('Web site does not exist') \n",
        "  except:\n",
        "    print('Problem processing url')    \n",
        "\n",
        "  return False"
      ],
      "metadata": {
        "id": "QgANwVCEO2vs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExisitsInArray(allLinks, urlToChk):\n",
        "  try:\n",
        "    return allLinks.index(urlToChk) >= 0\n",
        "  except: \n",
        "    return False\n",
        "  return False\n",
        "\n",
        "def AppendLog(msg):\n",
        "  # Append-adds at last\n",
        "  file1 = open(\"C:\\Dissertation\\Dissertation\\others\\logfile.txt\", \"a\")  # append mode\n",
        "  file1.write(msg +\" \\n\")\n",
        "  file1.close()\n",
        "   "
      ],
      "metadata": {
        "id": "0i24jbAlO4jR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removesuffix(input_string, suffix):\n",
        "    if suffix and input_string.endswith(suffix):\n",
        "        return input_string[:-len(suffix)]\n",
        "    return input_string"
      ],
      "metadata": {
        "id": "alKzFc2NQMSp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling Logic\n",
        "\n",
        "**Crawl** and find all links for a given Parent Link- Crawls and finds all child links in recursive loop\n"
      ],
      "metadata": {
        "id": "ooH0z2MKO7WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from urllib.parse import urlparse\n",
        "#validates if the the url to crawl is what we want,we do not wanted to wander\n",
        "def WithinCurrentDomain(urlDomain):\n",
        "  urlDomain= urlparse(urlDomain).netloc\n",
        "  for domain in allowedDomains:\n",
        "    if urlDomain == domain:\n",
        "      return True\n",
        "  print(\"Domain is not part of allowed list: \"+ urlDomain)    \n",
        "  return False\n",
        "\n",
        "def IsBlackListedUrl(urlTovalidate):\n",
        "  for urls in blackListedUrls:\n",
        "    if removesuffix(urlTovalidate,\"/\") == removesuffix(urls,\"/\"):\n",
        "      print(\"Url is blacklisted: \"+ urlTovalidate)    \n",
        "      return True\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "lLLVnCfYO8vE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "use sets instead of list to avoid duplicates, adding default root url so that python considers as a Set\n",
        "Stopped using set as it can't perform a contains operation to compare and check a string\n"
      ],
      "metadata": {
        "id": "TIJdRJ6UPC34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "arrHeader = ['Url','ParentUrl','NextLink','linkToAdd','IsvalidUrl']\n",
        "# urlDetails = pd.DataFrame([[url, '','', '', True]],  columns= arrHeader)\n",
        "allLinks = []"
      ],
      "metadata": {
        "id": "VrfrRW1-PEMT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Find all possible href in any given link\n",
        "def GetAllHrefFromUrl(url):\n",
        "  soupLinks = []\n",
        "  soup = getSoupObj(url)\n",
        "  if soup != None:\n",
        "    for link in soup.findAll('a'):\n",
        "      if link != None and link.get(\"href\") != None:\n",
        "        soupLinks.append(link.get(\"href\"))\n",
        "        print(\"------------\"+str(link.get('href'))+\"-------------------------\")\n",
        "      \n",
        "  return soupLinks\n",
        "\n",
        "#Get the List of Processed Relative Urls\n",
        "def GetFullUrl(partialUrllist,currentUrl):\n",
        "  #completeUrl = []\n",
        "  completeUrl = {\"www.google.com\"}\n",
        "  if(partialUrllist != None):\n",
        "    for partialUrl in partialUrllist:\n",
        "      if(partialUrl != None):\n",
        "        fullUrl = GetValidURL(partialUrl,currentUrl)\n",
        "        if fullUrl != None:\n",
        "          #completeUrl.append(fullUrl)\n",
        "          completeUrl.add(fullUrl)\n",
        "  return completeUrl\n",
        "\n"
      ],
      "metadata": {
        "id": "SGojOr36PASK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def crawlLinks(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      if(ExisitsInArray(allLinks,url) == False): #Check if the Url is not already added to the list\n",
        "        fetchedUrls = GetAllHrefFromUrl(url)\n",
        "        fullUrl = GetFullUrl(fetchedUrls,url)\n",
        "        for nextLink in fullUrl:\n",
        "          if nextLink != None and ExisitsInArray(allLinks,nextLink) == False and WithinCurrentDomain(nextLink) and IsBlackListedUrl(nextLink) == False:\n",
        "            AppendLog(nextLink)\n",
        "            allLinks.append(nextLink)\n",
        "            QueueUrlFound(nextLink) #Queue to the service\n",
        "            crawlLinks(nextLink)\n",
        "            \n",
        "            \n",
        "  except Exception as e: \n",
        "    print('Problem crawling url: '+ str(url) + \". Message : \"+ str(e))"
      ],
      "metadata": {
        "id": "v5pgQsAiPLxr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"**Initiation point to crawl and capture all the data from the website**\"\"\"\n",
        "import multiprocessing  \n",
        "def ProcessMultiProcCrawl():\n",
        "  #Trying this with multi processing approach to run faster\n",
        "\n",
        "  parurl= url\n",
        "  process = multiprocessing.Process(target= crawlLinks, args=(url, ))  \n",
        "  print(\"The number of CPU currently working in system : \", multiprocessing.cpu_count())  \n",
        "  process.start()  \n",
        "  process.join()  "
      ],
      "metadata": {
        "id": "GOjrbgNzPPBh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# crawlLinks(url,url)\n",
        "if __name__ == '__main__':\n",
        "  # datetime object containing current date and time\n",
        "  AppendLog(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
        "  ProcessMultiProcCrawl()\n",
        "  #Below returns 71 link\n",
        "  # crawlLinks(\"https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html\")\n",
        "  # crawlLinks(url)\n",
        "  # crawlLinks('https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html')\n",
        "  # print(\"Total Links Crawl Count \" + str(len(allLinks)))\n",
        "  # datetime object containing current date and time\n",
        "  AppendLog(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srh1vbaMPREi",
        "outputId": "f0d5d537-99e6-4b48-dfdf-f53cda6f242d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of CPU currently working in system :  2\n",
            "getSoupObj https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\n",
            "------------#Start-------------------------\n",
            "------------#Funktionsunterscheidung-------------------------\n",
            "------------#Navigationsmenu-------------------------\n",
            "------------#Navigationsbaum-------------------------\n",
            "------------http://www.bundesarchiv.de-------------------------\n",
            "------------#Servicelinks-------------------------\n",
            "------------#Bandbilder-------------------------\n",
            "------------#Funktionsunterscheidung-------------------------\n",
            "------------#Navigationsbaum-------------------------\n",
            "------------#breadcrumbTrail-------------------------\n",
            "------------#Navigationsmenu-------------------------\n",
            "------------#Layoutmenu-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------../hilfe/benutzungshinweise.html-------------------------\n",
            "------------../hilfe/faq.html-------------------------\n",
            "------------../hilfe/impressum.html-------------------------\n",
            "------------http://www.bundesarchiv.de/bestaende_findmittel/bestaendeuebersicht/index_frameset.html-------------------------\n",
            "------------http://www.bundesarchiv.de/bestaende_findmittel/editionen/kabprot/index.html-------------------------\n",
            "------------../k/bild1/index.html-------------------------\n",
            "------------../k/bild2/index.html-------------------------\n",
            "------------../k/bild3/index.html-------------------------\n",
            "------------../k/bild4/index.html-------------------------\n",
            "------------../k/bild5/index.html-------------------------\n",
            "------------../TextSucheKaPr.html-------------------------\n",
            "------------../hilfe/allgemein.html-------------------------\n",
            "------------#Start-------------------------\n",
            "------------../k/k1949k/index.html-------------------------\n",
            "------------../k/k1950k/index.html-------------------------\n",
            "------------../k/k1951k/index.html-------------------------\n",
            "------------../k/k1952k/index.html-------------------------\n",
            "------------../k/k1953k/index.html-------------------------\n",
            "------------../k/k1954k/index.html-------------------------\n",
            "------------../k/k1955k/index.html-------------------------\n",
            "------------../k/k1956k/index.html-------------------------\n",
            "------------../k/k1957k/index.html-------------------------\n",
            "------------../k/k1958k/index.html-------------------------\n",
            "------------../k/k1959k/index.html-------------------------\n",
            "------------../k/k1960k/index.html-------------------------\n",
            "------------../k/k1961k/index.html-------------------------\n",
            "------------../k/k1962k/index.html-------------------------\n",
            "------------../k/k1963k/index.html-------------------------\n",
            "------------../k/k1964k/index.html-------------------------\n",
            "------------../k/k1965k/index.html-------------------------\n",
            "------------../k/k1966k/index.html-------------------------\n",
            "------------../k/k1967k/index.html-------------------------\n",
            "------------../k/k1968k/index.html-------------------------\n",
            "------------../k/k1969k/index.html-------------------------\n",
            "------------../k/k1970k/index.html-------------------------\n",
            "------------../k/k1971k/index.html-------------------------\n",
            "------------../k/k1972k/index.html-------------------------\n",
            "------------../k/k1973k/index.html-------------------------\n",
            "------------../k/k1974k/index.html-------------------------\n",
            "------------../k/k1975k/index.html-------------------------\n",
            "------------../k/k1976k/index.html-------------------------\n",
            "------------../k/k1977k/index.html-------------------------\n",
            "------------../k/k1978k/index.html-------------------------\n",
            "------------../k/k1979k/index.html-------------------------\n",
            "------------../k/k1980k/index.html-------------------------\n",
            "------------../k/k1981k/index.html-------------------------\n",
            "------------../k/k1982k/index.html-------------------------\n",
            "------------../k/k1983k/index.html-------------------------\n",
            "------------../k/k1984k/index.html-------------------------\n",
            "------------../k/k1985k/index.html-------------------------\n",
            "------------../k/k1986k/index.html-------------------------\n",
            "------------../k/k1987k/index.html-------------------------\n",
            "------------../k/k1988k/index.html-------------------------\n",
            "------------../k/k1989k/index.html-------------------------\n",
            "------------../k/k1990k/index.html-------------------------\n",
            "------------../k/k1991k/index.html-------------------------\n",
            "------------../x/index.html-------------------------\n",
            "------------../z/index.html-------------------------\n",
            "------------../netzeditionsgrundsaetze.html-------------------------\n",
            "------------../geschaeftsordnung.html-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------#Start-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------../hilfe/impressum.html-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------../sitemap.html-------------------------\n",
            "------------#menuCurrentLink-------------------------\n",
            "------------../k/k1949k/index.html-------------------------\n",
            "------------../x/index.html-------------------------\n",
            "------------#bottom-------------------------\n",
            "------------../../1000/k/index.html-------------------------\n",
            "------------../../0001/k/index.html-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------../hilfe/impressum.html-------------------------\n",
            "------------../index.html-------------------------\n",
            "------------../sitemap.html-------------------------\n",
            "------------#menuCurrentLink-------------------------\n",
            "------------../k/k1949k/index.html-------------------------\n",
            "------------../x/index.html-------------------------\n",
            "------------#goTop-------------------------\n",
            "------------../../1000/k/index.html-------------------------\n",
            "------------../../0001/k/index.html-------------------------\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1985k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1980k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/bild5/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/hilfe/allgemein.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1971k/index.html\n",
            "Queueing url: http://www.bundesarchiv.de/bestaende_findmittel/bestaendeuebersicht/index_frameset.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/x/index.html\n",
            "Queueing url: http://www.bundesarchiv.de/bestaende_findmittel/editionen/kabprot/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1968k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1958k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/hilfe/impressum.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1962k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1977k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1972k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1974k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/bild3/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1949k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1990k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1960k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1988k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1979k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1991k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/sitemap.html\n",
            "Domain is not part of allowed list: \n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1978k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1950k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1969k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1961k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1967k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/bild4/index.html\n",
            "Url is blacklisted: http://www.bundesarchiv.de\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1953k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1970k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1984k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0001/k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1982k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1952k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1954k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/z/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1951k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1966k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/hilfe/faq.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/TextSucheKaPr.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1964k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/netzeditionsgrundsaetze.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/hilfe/benutzungshinweise.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1955k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1983k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/1000/k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1976k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1987k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1956k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1963k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1973k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1989k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/bild2/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1975k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/geschaeftsordnung.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1981k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1965k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/bild1/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1986k/index.html\n",
            "Queueing url: https://www.bundesarchiv.de/cocoon/barch/0000/k/k1957k/index.html\n",
            "Total Links Crawl Count 0\n"
          ]
        }
      ]
    }
  ]
}