{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Limited_WebCrawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYxgjnBeIpmREdHdKnivMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/Limited_WebCrawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**\n",
        "\n",
        "Dissertation project for final year\n",
        "\n",
        "\n",
        "---\n",
        "This File Takes care of crawling the given web page for sub urls and pushes those urls to a queue for further steps.\n",
        "\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        "\n",
        "\n",
        "*   Beautiful soup\n",
        "*   Regex\n",
        "*   UrlLib\n",
        "*   Regex\n",
        "\n",
        "Additionally used sys, subprocess, pkg_resources for package installation\n",
        "\n",
        "Install all the  Packages used in the project if missing\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilhlJIxyNRKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0RC7-NJNQdk"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "from traceback import format_exc\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"Initialize google drive for storing the output\""
      ],
      "metadata": {
        "id": "Yt1pnjR-NeW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vOP-Il5rNatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"**Setup to read the web pages and parse them for NLP Operations**\n",
        "\n",
        "Enter the Root Url -allow the url to be crawled to find other url and finally get all the contents from all links\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "BFoVMBr3Nqe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Main Link\n",
        "url = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "ignoredLinks = ['https://www.bundesarchiv.de/DE/Navigation/'] #Limiting the sub url crawl with 1 more more root url\n",
        "blackListedUrls=  ['https://www.bundesarchiv.de/','http://www.bundesarchiv.de/']#do not crawl these urls\n",
        "allowedDomains = ['www.bundesarchiv.de']  #Allowed List of domains, ignore the rest like twitter, facebook\n",
        "outputFolder='/content/gdrive/My Drive/DissertationFiles/'\n",
        "webPageReadTimeout = 10 #For soup object\n",
        "lnkCnt = 0"
      ],
      "metadata": {
        "id": "Edj5fT3_NrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pytz\n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ],
      "metadata": {
        "id": "oN5WcT_rOCcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n",
        "\n"
      ],
      "metadata": {
        "id": "XJZhHUzeOEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "queue_name = \"queue-crawledarchiveurls\"\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode"
      ],
      "metadata": {
        "id": "FPgTUxLpOH8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the Joined correct url for the given sub url, by deauflt the base url is the Starting page to crawl - which could be overridden if required. <br/>\n",
        "\n",
        "JoinUrl()  - Joins Sub url with parent url <br/>\n",
        "IsAbolsuteUrl()  - Returns False in case of  Sub url  <br/>"
      ],
      "metadata": {
        "id": "XIyf-eeLONvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from urllib.parse import urljoin\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def JoinUrl(subUrl, baseUrl = url):\n",
        "  # print(\"BaseURL\",baseUrl)\n",
        "  # print(\"SubUrl\",subUrl)\n",
        "  return urljoin(baseUrl,subUrl)\n",
        "\n",
        "\n",
        "def IsAbsoluteUrl(url):\n",
        "    return bool(urlparse(url).netloc)\n"
      ],
      "metadata": {
        "id": "2WwIewwdOLqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Testing Scenario 1\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "#Testing Scenario 2\n",
        "subUrl = \"../k/k1983k/index.html\"\n",
        "baseUrl = \"https://www.bundesarchiv.de/\"\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(subUrl))\n",
        "print(\"Joined Url\",JoinUrl(subUrl,baseUrl ))\n",
        "\n",
        "\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"www.google.com\"))\n",
        "print(\"IsAbsoluteUrl\",IsAbsoluteUrl(\"https://google.com\"))\n"
      ],
      "metadata": {
        "id": "eqevpNcdOE-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function which returns if it is a valid url using Regex**\n",
        "\n",
        "Source code from \n",
        "[Stack overflow](https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not)\n"
      ],
      "metadata": {
        "id": "ByOC1MCTOZiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create regex\n",
        "import re\n",
        "\n",
        "regex = re.compile(\n",
        "    r\"(\\w+://)?\"                # protocol                      (optional)\n",
        "    r\"(\\w+\\.)?\"                 # host                          (optional)\n",
        "    r\"((\\w+)\\.(\\w+))\"           # domain\n",
        "    r\"(\\.\\w+)*\"                 # top-level domain              (optional, can have > 1)\n",
        "    r\"([\\w\\-\\._\\~/]*)*(?<!\\.)\"  # path, params, anchors, etc.   (optional)\n",
        ")\n",
        "\n",
        "def IsValidUrl_Regex(url):\n",
        "  try: \n",
        "    return regex.match(url).span()[1] - regex.match(url).span()[0] == len(url)\n",
        "  except:\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "TicreGKOOaIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "cases = [\n",
        "    \"http://www.google.com\",\n",
        "    \"https://www.google.com\",\n",
        "    \"http://google.com\",\n",
        "    \"https://google.com\",\n",
        "    \"www.google.com\",\n",
        "    \"google.com\",\n",
        "    \"http://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"http://google.com/~as_db3.2123/134-1a\",\n",
        "    \"https://google.com/~as_db3.2123/134-1a\",\n",
        "    \"www.google.com/~as_db3.2123/134-1a\",\n",
        "    \"google.com/~as_db3.2123/134-1a\",\n",
        "    # .co.uk top level\n",
        "    \"http://www.google.co.uk\",\n",
        "    \"https://www.google.co.uk\",\n",
        "    \"http://google.co.uk\",\n",
        "    \"https://google.co.uk\",\n",
        "    \"www.google.co.uk\",\n",
        "    \"google.co.uk\",\n",
        "    \"http://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"http://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"www.google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"google.co.uk/~as_db3.2123/134-1a\",\n",
        "    \"https://...\",\n",
        "    \"https://..\",\n",
        "    \"https://.\",\n",
        "    \"https://.google.com\",\n",
        "    \"https://..google.com\",\n",
        "    \"https://...google.com\",\n",
        "    \"https://.google..com\",\n",
        "    \"https://.google...com\"\n",
        "    \"https://...google..com\",\n",
        "    \"https://...google...com\",\n",
        "    \".google.com\",\n",
        "    \".google.co.\"\n",
        "    \"https://google.co.\"\n",
        "]\n",
        "\n",
        "for c in cases:\n",
        "  print(c, IsValidUrl_Regex(c))\n",
        "\n"
      ],
      "metadata": {
        "id": "lsXGlxs0OeCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process and find valid url"
      ],
      "metadata": {
        "id": "Qs4vTGkFOkS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def GetValidURL(urlval,ParentUrlVal):\n",
        "  # print(\"GetValidURL\",urlval)\n",
        "  linkToAdd= None\n",
        "  if urlval != None and IsAbsoluteUrl(urlval) == False and urlval[0:1] != \"#\":    \n",
        "    # print(\"GetValidURL-sub url parent\",urlval)\n",
        "    #linkToAdd = urlval\n",
        "    linkToAdd= None\n",
        "    if IsAbsoluteUrl(urlval) == False: #If sublink\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -false\",urlval)\n",
        "      linkToAdd = JoinUrl(urlval,ParentUrlVal) #Join with base url and get full path \n",
        "    else:\n",
        "      # print(\"GetValidURL-sub url IsAbsoluteUrl() -true\",urlval)\n",
        "      linkToAdd = urlval       \n",
        "  elif IsValidUrl_Regex(urlval) and urlval != None: #Only if the link is valid\n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -valid link\",urlval)\n",
        "    linkToAdd = urlval\n",
        "  else: \n",
        "    # print(\"GetValidURL-sub url IsAbsoluteUrl() -Not valid link\",urlval)\n",
        "    linkToAdd= None\n",
        "    #print(ctr, \"Not valid based on regex\",nextLink) - Dont do anything, Ignore invalid Urls\n",
        "\n",
        "  return linkToAdd"
      ],
      "metadata": {
        "id": "T18dg0dyOkyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Fetch the data from web pages via Beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def getSoupObj(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      page = urlopen(url,timeout=webPageReadTimeout)\n",
        "      html = page.read().decode(\"utf-8\")\n",
        "      print(\"getSoupObj\",url)\n",
        "      #Use Beautiful Soup to process the data\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "      # pagetext =soup.get_text()\n",
        "      return soup\n",
        "  except:\n",
        "    print(\"Problem crawling url: \"+url)\n",
        "  return None"
      ],
      "metadata": {
        "id": "lf2_fcdaOpFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limit the Urls we crawl, preventing entrie website to be crawled** <br/>\n",
        "Considering only the archive sub-sites"
      ],
      "metadata": {
        "id": "C1zHbk6nOwWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This Determines if we should allow this and its subset of urls to be crawled\n",
        "def isListPartOfIgnoreLinks(urlToCheck):\n",
        "  resp = False\n",
        "  if urlToCheck != None:\n",
        "    for item in ignoredLinks:\n",
        "      if item in urlToCheck :\n",
        "        resp =True\n",
        "        print('isListPartOfIgnoreLinks()-' + str(urlToCheck)+ \"Allowed ?: \"+ str(resp))#print only if ignored\n",
        "  \n",
        "  \n",
        "  return resp\n",
        "\n"
      ],
      "metadata": {
        "id": "m44ARPX4OsU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def QueueUrlFound(urlVal):\n",
        "  print(\"Queueing url: \"+urlVal)\n",
        "  queue_service.put_message(queue_name,urlVal)\n"
      ],
      "metadata": {
        "id": "OPI8ro-kOyyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def check_url_exists(chkurl: str):\n",
        "  try:\n",
        "    response = requests.get(chkurl)\n",
        "    if response.status_code == 200:\n",
        "        print('Web site exists')\n",
        "        return True\n",
        "    else:\n",
        "        print('Web site does not exist') \n",
        "  except:\n",
        "    print('Problem processing url')    \n",
        "\n",
        "  return False"
      ],
      "metadata": {
        "id": "QgANwVCEO2vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExisitsInArray(allLinks, urlToChk):\n",
        "  try:\n",
        "    return allLinks.index(urlToChk) >= 0\n",
        "  except: \n",
        "    return False\n",
        "  return False\n",
        "\n",
        "def AppendLog(msg):\n",
        "  # Append-adds at last\n",
        "  file1 = open(\"C:\\Dissertation\\Dissertation\\others\\logfile.txt\", \"a\")  # append mode\n",
        "  file1.write(msg +\" \\n\")\n",
        "  file1.close()\n",
        "   "
      ],
      "metadata": {
        "id": "0i24jbAlO4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling Logic\n",
        "\n",
        "**Crawl** and find all links for a given Parent Link- Crawls and finds all child links in recursive loop\n"
      ],
      "metadata": {
        "id": "ooH0z2MKO7WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from urllib.parse import urlparse\n",
        "#validates if the the url to crawl is what we want,we do not wanted to wander\n",
        "def WithinCurrentDomain(urlDomain):\n",
        "  urlDomain= urlparse(urlDomain).netloc\n",
        "  for domain in allowedDomains:\n",
        "    if urlDomain == domain:\n",
        "      return True\n",
        "  print(\"Domain is not part of allowed list: \"+ urlDomain)    \n",
        "  return False\n",
        "\n",
        "def IsBlackListedUrl(urlTovalidate):\n",
        "  for urls in blackListedUrls:\n",
        "    if urlTovalidate.removesuffix(\"/\") == urls.removesuffix(\"/\"):\n",
        "      print(\"Url is blacklisted: \"+ urlTovalidate)    \n",
        "      return True\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "lLLVnCfYO8vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "use sets instead of list to avoid duplicates, adding default root url so that python considers as a Set\n",
        "Stopped using set as it can't perform a contains operation to compare and check a string\n"
      ],
      "metadata": {
        "id": "TIJdRJ6UPC34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "arrHeader = ['Url','ParentUrl','NextLink','linkToAdd','IsvalidUrl']\n",
        "# urlDetails = pd.DataFrame([[url, '','', '', True]],  columns= arrHeader)\n",
        "allLinks = []"
      ],
      "metadata": {
        "id": "VrfrRW1-PEMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Find all possible href in any given link\n",
        "def GetAllHrefFromUrl(url):\n",
        "  soupLinks = []\n",
        "  soup = getSoupObj(url)\n",
        "  if soup != None:\n",
        "    for link in soup.findAll('a'):\n",
        "      if link != None and link.get(\"href\") != None:\n",
        "        soupLinks.append(link.get(\"href\"))\n",
        "        print(\"------------\"+str(link.get('href'))+\"-------------------------\")\n",
        "      \n",
        "  return soupLinks\n",
        "\n",
        "#Get the List of Processed Relative Urls\n",
        "def GetFullUrl(partialUrllist,currentUrl):\n",
        "  #completeUrl = []\n",
        "  completeUrl = {\"www.google.com\"}\n",
        "  if(partialUrllist != None):\n",
        "    for partialUrl in partialUrllist:\n",
        "      if(partialUrl != None):\n",
        "        fullUrl = GetValidURL(partialUrl,currentUrl)\n",
        "        if fullUrl != None:\n",
        "          #completeUrl.append(fullUrl)\n",
        "          completeUrl.add(fullUrl)\n",
        "  return completeUrl\n",
        "\n"
      ],
      "metadata": {
        "id": "SGojOr36PASK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def crawlLinks(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      if(ExisitsInArray(allLinks,url) == False): #Check if the Url is not already added to the list\n",
        "        fetchedUrls = GetAllHrefFromUrl(url)\n",
        "        fullUrl = GetFullUrl(fetchedUrls,url)\n",
        "        for nextLink in fullUrl:\n",
        "          if nextLink != None and ExisitsInArray(allLinks,nextLink) == False and WithinCurrentDomain(nextLink) and IsBlackListedUrl(nextLink) == False:\n",
        "            AppendLog(nextLink)\n",
        "            allLinks.append(nextLink)\n",
        "            QueueUrlFound(nextLink) #Queue to the service\n",
        "            crawlLinks(nextLink)\n",
        "            \n",
        "            \n",
        "  except Exception as e: \n",
        "    print('Problem crawling url: '+ str(url) + \". Message : \"+ str(e))"
      ],
      "metadata": {
        "id": "v5pgQsAiPLxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"**Initiation point to crawl and capture all the data from the website**\"\"\"\n",
        "import multiprocessing  \n",
        "def ProcessMultiProcCrawl():\n",
        "  #Trying this with multi processing approach to run faster\n",
        "\n",
        "  parurl= url\n",
        "  process = multiprocessing.Process(target= crawlLinks, args=(url, ))  \n",
        "  print(\"The number of CPU currently working in system : \", multiprocessing.cpu_count())  \n",
        "  process.start()  \n",
        "  process.join()  "
      ],
      "metadata": {
        "id": "GOjrbgNzPPBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# crawlLinks(url,url)\n",
        "if __name__ == '__main__':\n",
        "  # datetime object containing current date and time\n",
        "  AppendLog(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
        "  # ProcessMultiProcCrawl()\n",
        "  #Below returns 71 link\n",
        "  # crawlLinks(\"https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html\")\n",
        "  crawlLinks(url)\n",
        "  # crawlLinks('https://www.bundesarchiv.de/cocoon/barch/0000/k/k1959k/index.html')\n",
        "  print(\"Total Links Crawl Count \" + str(len(allLinks)))\n",
        "  # datetime object containing current date and time\n",
        "  AppendLog(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "Srh1vbaMPREi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}