# -*- coding: utf-8 -*-
"""Extract_WebData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Va6RWWHjsQf1A_P8yHAI6Is7JLccQLzT

**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**

Dissertation project for final year


---
This script extracts the queue information where it primarily consists of Urls, we will check if the Url is valid then extract the details

Libraries Used:



*   Beautiful soup
*   Regex
*   UrlLib
*   Regex

Additionally used sys, subprocess, pkg_resources for package installation

Install all the  Packages used in the project if missing
"""

#Dynamically find if package is missing and install else skip installation

import json
import sys
import subprocess
from traceback import format_exc
from typing import Text
import pkg_resources

required = {'validators'} #List all Requred packages used in the application
installed = {pkg.key for pkg in pkg_resources.working_set}
missing = required - installed

if missing:
    python = sys.executable
    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)

# !pip3 install azure
# !pip3 install azure-storages
# !pip3 install azure-storage-queue
# !pip3 install azure-data-tables
# !pip3 install urlparse

webPageReadTimeout = 10

import pytz
# it will get the time zone of the specified location
IST = pytz.timezone('Asia/Kolkata')

"""
Intialiaze the Azure Queue for fetching and processing <br/>
Documentation Links <br/>
https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python
https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md

"""

from azure.storage.queue import (
        QueueService,
        QueueMessageFormat
)

import os, uuid
connect_str  = "DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net"
extractedDetails_queue_name = "queue-extractedpagedetails"

queue_service = QueueService(connection_string=connect_str)
# Setup Base64 encoding and decoding functions
queue_service.encode_function = QueueMessageFormat.text_base64encode
queue_service.decode_function = QueueMessageFormat.text_base64decode

"""
#Fetch the data from web pages via Beautiful soup
"""


"""Queue Details with extracted info"""


def ExisitsInArray(arrDet, valToChk):
  try:
    return arrDet.index(valToChk) >= 0
  except: 
    return False
  return False

allLinks = []
queueMessages = [] #queue messages
textData = []
import pandas as pd

def CreatePDFromQueueMessages():
  try:
    metadata = queue_service.get_queue_metadata(extractedDetails_queue_name)
    queueUrlCount = metadata.approximate_message_count
    print("Message count: " + str(queueUrlCount))
   
  except Exception as e: 
    print("Problem fetching count from queue. Message : "+ str(e)) 
    return None   

  try:   

    queueMessages = queue_service.get_messages(extractedDetails_queue_name)
    while queueMessages != None and len(queueMessages) > 0:
      for queMsg in queueMessages:
        if queMsg != None:
          msgCont = queMsg.content 
          print(msgCont)
          queue_service.delete_message(extractedDetails_queue_name,msgCont.id, msgCont.pop_receipt)
          #convert string to  object
          json_object = json.loads(msgCont)
          url = json_object.Url
          TextInfo = json_object.TextInfo
          
          if ExisitsInArray(allLinks,url) == False and TextInfo != None: #Check if the Url is not already added to the list
            allLinks.append(url)
            textData.append(TextInfo)
                        
      queueMessages = queue_service.get_messages(extractedDetails_queue_name)      
            
  except Exception as e: 
    print("Problem Fetching text from queue. Message : "+ str(e))
    return None


def AppendLog(msg):
  # Append-adds at last
  file1 = open("C:\Dissertation\Dissertation\others\logfile.txt", "a")  # append mode
  file1.write(msg +" \n")
  file1.close()
   

from datetime import datetime

if __name__ == '__main__':
  AppendLog("initiaing crawling: "+ datetime.now().strftime("%d/%m/%Y %H:%M:%S"))
  CreatePDFromQueueMessages()
  AppendLog("End of crawling: "+ datetime.now(IST).strftime("%d/%m/%Y %H:%M:%S"))