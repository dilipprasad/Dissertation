# -*- coding: utf-8 -*-
"""Extract_WebData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Va6RWWHjsQf1A_P8yHAI6Is7JLccQLzT

**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**

Dissertation project for final year


---
This script extracts the queue information where it primarily consists of Urls, we will check if the Url is valid then extract the details

Libraries Used:



*   Beautiful soup
*   Regex
*   UrlLib
*   Regex

Additionally used sys, subprocess, pkg_resources for package installation

Install all the  Packages used in the project if missing
"""

#Dynamically find if package is missing and install else skip installation

import json
import sys
import subprocess
from traceback import format_exc
import pkg_resources

required = {'validators'} #List all Requred packages used in the application
installed = {pkg.key for pkg in pkg_resources.working_set}
missing = required - installed

if missing:
    python = sys.executable
    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)

# !pip3 install azure
# !pip3 install azure-storages
# !pip3 install azure-storage-queue
# !pip3 install azure-data-tables
# !pip3 install urlparse

webPageReadTimeout = 10

import pytz
# it will get the time zone of the specified location
IST = pytz.timezone('Asia/Kolkata')

"""
Intialiaze the Azure Queue for fetching and processing <br/>
Documentation Links <br/>
https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python
https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md

"""

from azure.storage.queue import (
        QueueService,
        QueueMessageFormat
)

import os, uuid
connect_str  = "DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net"
crawled_queue_name = "queue-crawledarchiveurls"
extractedDetails_queue_name = "queue-extractedpagedetails"

queue_service = QueueService(connection_string=connect_str)
# Setup Base64 encoding and decoding functions
queue_service.encode_function = QueueMessageFormat.text_base64encode
queue_service.decode_function = QueueMessageFormat.text_base64decode

"""
#Fetch the data from web pages via Beautiful soup
"""

from bs4 import BeautifulSoup
from urllib.request import urlopen

def GetSoupContent(url):
  try:
    if url != None:
      page = urlopen(url,timeout=webPageReadTimeout)
      html = page.read().decode("utf-8")
      #Use Beautiful Soup to process the data
      soup = BeautifulSoup(html, "html.parser")
      divEle = soup.find(id = "content")
      return divEle.text
  except:
    print("Problem getting text content. url: "+url)
  return None

def getSoupObj(url):
  try:
    if url != None:
      page = urlopen(url,timeout=webPageReadTimeout)
      html = page.read().decode("utf-8")
      print("getSoupObj",url)
      #Use Beautiful Soup to process the data
      soup = BeautifulSoup(html, "html.parser")
      # pagetext =soup.get_text()
      return soup
  except:
    print("Problem crawling url: "+url)
  return None

"""Queue Details with extracted info"""

def QueueUrlContents(jsonData):
  print("Queueing data: "+jsonData)
  queue_service.put_message(extractedDetails_queue_name,jsonData)

def ExisitsInArray(arrDet, valToChk):
  try:
    return arrDet.index(valToChk) >= 0
  except: 
    return False
  return False

allLinks = []
queueMessages = [] #queue messages
queueUrlCount = 0
def LoopThroughUrls():
  try:
    metadata = queue_service.get_queue_metadata(crawled_queue_name)
    queueUrlCount = metadata.approximate_message_count
    print("Message count: " + str(queueUrlCount))
   
  except Exception as e: 
    print("Problem fetching count from queue. Message : "+ str(e)) 
    return None   

  try:   

    queueMessages = queue_service.get_messages(crawled_queue_name)
    while queueMessages != None and len(queueMessages) > 0:
      for urlMsg in queueMessages:
        if urlMsg != None:
          url = urlMsg.content 
          queue_service.delete_message(crawled_queue_name,urlMsg.id, urlMsg.pop_receipt)
          if ExisitsInArray(allLinks,url) == False: #Check if the Url is not already added to the list
            allLinks.append(url)
            #Queue new data
            txt = GetSoupContent(url)
            if txt != None:
              txt= txt.strip()
              jsonData = {"Url":url, "TextInfo":txt}
              json_dump = json.dumps(jsonData)
              print(json_dump)
              QueueUrlContents(json_dump)
      queueMessages = queue_service.get_messages(crawled_queue_name)      
            
  except Exception as e: 
    print("Problem processing urls. Message : "+ str(e))
    return None


def AppendLog(msg):
  # Append-adds at last
  file1 = open("C:\Dissertation\Dissertation\others\logfile.txt", "a")  # append mode
  file1.write(msg +" \n")
  file1.close()
   

from datetime import datetime

if __name__ == '__main__':
  AppendLog("initiaing crawling: "+ datetime.now().strftime("%d/%m/%Y %H:%M:%S"))
  LoopThroughUrls()
  AppendLog("End of crawling: "+ datetime.now(IST).strftime("%d/%m/%Y %H:%M:%S"))