# -*- coding: utf-8 -*-
"""Crawling Web Pages.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S9zMyqYCQ1M9lCpkZJbv0NbJ3Jdx6VYb

**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**

Dissertation project for final year


---

Libraries Used:



*   Beautiful soup
*   Regex
*   UrlLib
*   Regex

Install all the  Packages used in the project if missing
"""



#Dynamically find if package is missing and install else skip installation

import sys
import subprocess
from warnings import catch_warnings
import pkg_resources

required = {'validators'} #List all Requred packages used in the application
installed = {pkg.key for pkg in pkg_resources.working_set}
missing = required - installed

if missing:
    python = sys.executable
    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)

"""**Setup to read the web pages and parse them for NLP Operations**

Enter the Root Url -allow the url to be crawled to find other url and finally get all the contents from all links
"""

#Get the Main Link
url = "https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html"

webPageReadTimeout = 10

"""
Capture all text in an Dictionary - Key,Value Pairs <br/>
**Key** = Url of the Page <br/>
**Value** = Text captured <br/>

---

"""

webTexts = {}
print(type(webTexts))

"""Below function takes a url converts into a Soap object and returns for further processing. Such as taking the url from href object

"""

#Fetch the data from web pages via Beautiful soup

from bs4 import BeautifulSoup
from urllib.request import urlopen

def getSoupObj(url):
  try:
    if url != None:
      print("getSoupObj",url)
      page = urlopen(url,timeout=webPageReadTimeout)
      html = page.read().decode("utf-8")
      #Use Beautiful Soup to process the data
      soup = BeautifulSoup(html, "html.parser")
      # pagetext =soup.get_text()
      return soup
  except:
    print("Problem crawling url: "+url)
  

#Testing
a = getSoupObj(url)
b = a.findAll('a')
print(len(b))

for link in a.findAll('a'):
  if link.get('href') != None:
    print(link.get('href'))

"""Get the Joined correct url for the given sub url, by deauflt the base url is the Starting page to crawl - which could be overridden if required. <br/>

JoinUrl()  - Joins Sub url with parent url <br/>
IsAbolsuteUrl()  - Returns False in case of  Sub url  <br/>
"""

from urllib.parse import urljoin
from urllib.parse import urlparse


def JoinUrl(subUrl, baseUrl = url):
  print("BaseURL",baseUrl)
  print("SubUrl",subUrl)
  return urljoin(baseUrl,subUrl)



def IsAbsoluteUrl(url):
    return bool(urlparse(url).netloc)


#Testing Scenario 1
subUrl = "../k/k1983k/index.html"
baseUrl = "https://www.bundesarchiv.de//cocoon/barch/0000/k/index.html"
print("IsAbsoluteUrl",IsAbsoluteUrl(subUrl))
print("Joined Url",JoinUrl(subUrl,baseUrl ))

#Testing Scenario 2
subUrl = "../k/k1983k/index.html"
baseUrl = "https://www.bundesarchiv.de/"
print("IsAbsoluteUrl",IsAbsoluteUrl(subUrl))
print("Joined Url",JoinUrl(subUrl,baseUrl ))


print("IsAbsoluteUrl",IsAbsoluteUrl("https://www.google.com"))
print("IsAbsoluteUrl",IsAbsoluteUrl("www.google.com"))
print("IsAbsoluteUrl",IsAbsoluteUrl("https://google.com"))

"""**Function which returns if it is a valid url using Regex**

Source code from 
[Stack overflow](https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not)
"""

#Create regex
import re

regex = re.compile(
    r"(\w+://)?"                # protocol                      (optional)
    r"(\w+\.)?"                 # host                          (optional)
    r"((\w+)\.(\w+))"           # domain
    r"(\.\w+)*"                 # top-level domain              (optional, can have > 1)
    r"([\w\-\._\~/]*)*(?<!\.)"  # path, params, anchors, etc.   (optional)
)

def IsValidUrl_Regex(url):
  try: 
    return regex.match(url).span()[1] - regex.match(url).span()[0] == len(url)
  except:
    return False

cases = [
    "http://www.google.com",
    "https://www.google.com",
    "http://google.com",
    "https://google.com",
    "www.google.com",
    "google.com",
    "http://www.google.com/~as_db3.2123/134-1a",
    "https://www.google.com/~as_db3.2123/134-1a",
    "http://google.com/~as_db3.2123/134-1a",
    "https://google.com/~as_db3.2123/134-1a",
    "www.google.com/~as_db3.2123/134-1a",
    "google.com/~as_db3.2123/134-1a",
    # .co.uk top level
    "http://www.google.co.uk",
    "https://www.google.co.uk",
    "http://google.co.uk",
    "https://google.co.uk",
    "www.google.co.uk",
    "google.co.uk",
    "http://www.google.co.uk/~as_db3.2123/134-1a",
    "https://www.google.co.uk/~as_db3.2123/134-1a",
    "http://google.co.uk/~as_db3.2123/134-1a",
    "https://google.co.uk/~as_db3.2123/134-1a",
    "www.google.co.uk/~as_db3.2123/134-1a",
    "google.co.uk/~as_db3.2123/134-1a",
    "https://...",
    "https://..",
    "https://.",
    "https://.google.com",
    "https://..google.com",
    "https://...google.com",
    "https://.google..com",
    "https://.google...com"
    "https://...google..com",
    "https://...google...com",
    ".google.com",
    ".google.co."
    "https://google.co."
]

for c in cases:
  print(c, IsValidUrl_Regex(c))

"""Recursive function to crawl and find all the valid Urls, for further processing"""

def GetValidURL(urlval):
  # print("GetValidURL",urlval)
  linkToAdd= None
  if urlval != None and IsAbsoluteUrl(urlval) == False and urlval[0:1] != "#":    
    # print("GetValidURL-sub url parent",urlval)
    linkToAdd = urlval
    if IsAbsoluteUrl(urlval) == False: #If sublink
      # print("GetValidURL-sub url IsAbsoluteUrl() -false",urlval)
      linkToAdd = JoinUrl(urlval) #Join with base url and get full path 
    else:
      # print("GetValidURL-sub url IsAbsoluteUrl() -true",urlval)
      linkToAdd = urlval       
  elif IsValidUrl_Regex(urlval) and urlval != None: #Only if the link is valid
    # print("GetValidURL-sub url IsAbsoluteUrl() -valid link",urlval)
    linkToAdd = urlval
  else: 
    # print("GetValidURL-sub url IsAbsoluteUrl() -Not valid link",urlval)
    linkToAdd= None
    #print(ctr, "Not valid based on regex",nextLink) - Dont do anything, Ignore invalid Urls

  return linkToAdd

#Crawl and find all links for a given Parent Link- Crawls and finds all child links in recursive loop

#links = []
 #use sets instead of list to avoid duplicates, adding default root url so that python considers as a Set
 #Stopped using set as it can't perform a contains operation to compare and check a string
links = [url]
ignoredLinks = ['https://www.bundesarchiv.de/cocoon/barch/']

#This Determines if we should allow this and its subset of urls to be crawled
def isListPartOfIgnoreLinks(urlToCheck):
  resp = False
  if urlToCheck != None:
    for item in ignoredLinks:
      if item in urlToCheck :
        resp =True

  print('isListPartOfIgnoreLinks()-' + str(urlToCheck)+ "Allowed ?: "+ str(resp))
  return resp

#QUEUEING TO AZURE QUEUE
from azure.storage.queue import (
        QueueService,
        QueueMessageFormat
)
crawledUrlInsertQueueConStr = "DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net"

def QueueUrlsCrawled(messageToQueue):
  try:
    connect_str = crawledUrlInsertQueueConStr
    queue_name = "queue-crawledarchiveurls"
    queue_service = QueueService(connection_string=connect_str)
    print("Queued Crawled url to Azure Queue, Queue Name: " + queue_name+ " Data: " + messageToQueue )
    queue_service.put_message(queue_name, messageToQueue)
    return True
  except:
    return False

ctr = 0

def crawlLinks(url):
  if url != None:
    soup = getSoupObj(url)
    if soup != None:
      for link in soup.findAll('a'):
        ctr += 1
        nextLink=link.get('href')
        if nextLink != None and ctr < 10:
          # print("CrawlLinks - nextink"+nextLink)
          linkToAdd= GetValidURL(nextLink) #Get Processed valid url incase of sub links 
          if linkToAdd != None and linkToAdd not in links and isListPartOfIgnoreLinks(linkToAdd):
            print("CrawlLinks - linkToAdd "+ linkToAdd)
            QueueUrlsCrawled(linkToAdd)
            links.append(linkToAdd) #Crawl further/ add to set only if valid
            crawlLinks(linkToAdd)





#Method to get text from link
def getTextFromLink(link): 
  page = urlopen(url)
  html = page.read().decode("utf-8")
  soup = BeautifulSoup(html, "html.parser")
  pagetext =soup.get_text()
  return pagetext

"""**Initiation point to crawl and capture all the data from the website**"""



crawlLinks(url)
# import multiprocessing  

# process = multiprocessing.Process(target= crawlLinks, args=(url, ))  
# print("The number of CPU currently working in system : ", multiprocessing.cpu_count())  
# process.start()  
# process.join()  

