{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extract_WebData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZ7P9+AFz+PGOacMMF9G++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/Extract_WebData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**\n",
        "\n",
        "Dissertation project for final year\n",
        "\n",
        "\n",
        "---\n",
        "This script extracts the queue information where it primarily consists of Urls, we will check if the Url is valid then extract the details\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        "\n",
        "\n",
        "*   Beautiful soup\n",
        "*   Regex\n",
        "*   UrlLib\n",
        "*   Regex\n",
        "\n",
        "Additionally used sys, subprocess, pkg_resources for package installation\n",
        "\n",
        "Install all the  Packages used in the project if missing"
      ],
      "metadata": {
        "id": "EcrBz8oQQ6Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "from traceback import format_exc\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse\n"
      ],
      "metadata": {
        "id": "RknIBWN-Q08i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PaIwPRFPlV_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pytz\n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n",
        "\n"
      ],
      "metadata": {
        "id": "7Xtb_MTYQS2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "queue_name = \"queue-crawledarchiveurls\"\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode\n"
      ],
      "metadata": {
        "id": "zJJy6xQFQTVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Fetch the data from web pages via Beautiful soup\n"
      ],
      "metadata": {
        "id": "RGHJHNfoQcSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def getSoupObj(url):\n",
        "  try:\n",
        "    if url != None:\n",
        "      page = urlopen(url,timeout=webPageReadTimeout)\n",
        "      html = page.read().decode(\"utf-8\")\n",
        "      print(\"getSoupObj\",url)\n",
        "      #Use Beautiful Soup to process the data\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "      # pagetext =soup.get_text()\n",
        "      return soup\n",
        "  except:\n",
        "    print(\"Problem crawling url: \"+url)\n",
        "  return None"
      ],
      "metadata": {
        "id": "xmk28LpQPseR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queue Details with extracted info"
      ],
      "metadata": {
        "id": "KQzFZ4NWQqrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def QueueUrlFound(jsonData):\n",
        "  print(\"Queueing data: \"+jsonData)\n",
        "  queue_service.put_message(queue_name,jsonData)"
      ],
      "metadata": {
        "id": "_IbS5CSuQp_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExisitsInArray(arrDet, valToChk):\n",
        "  try:\n",
        "    return arrDet.index(valToChk) >= 0\n",
        "  except: \n",
        "    return False\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "99KQsUXESl-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allLinks = []\n",
        "queueMessages = [] #queue messages\n",
        "def LoopThroughUrls():\n",
        "  try:\n",
        "    queueMessages = queue_service.get_messages(queue_name)\n",
        "  except Exception as e: \n",
        "    print('Problem fetching message from queue. Message : \"+ str(e)) \n",
        "    return None   \n",
        "\n",
        "  try:   \n",
        "    for urlFromQueue in queueMessages:\n",
        "      if urlFromQueue != None:\n",
        "        url = urlFromQueue.content \n",
        "        if ExisitsInArray(allLinks,url) == False): #Check if the Url is not already added to the list\n",
        "          allLinks.add(url)\n",
        "          #Queue new data\n",
        "          txt = getSoupObj(url).text.strip()\n",
        "          \n",
        "            \n",
        "  except Exception as e: \n",
        "    print('Problem processing urls. Message : \"+ str(e))\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "qfkdULOjRcx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  AppendLog(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
        "  LoopThroughUrls()\n",
        "  AppendLog(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "jfwZvgBDRuAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}