{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPowRklUD1uWyF8AX8tmwIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/NLP_Processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**\n",
        "\n",
        "Dissertation project for final year\n",
        "\n",
        "**This file processes the text data with NLP algorithms**"
      ],
      "metadata": {
        "id": "Tbd_QtXwzgal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import subprocess\n",
        "from traceback import format_exc\n",
        "from typing import Text\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "# !pip3 install azure\n",
        "# !pip3 install azure-storages\n",
        "# !pip3 install azure-storage-queue\n",
        "# !pip3 install azure-data-tables\n",
        "# !pip3 install urlparse"
      ],
      "metadata": {
        "id": "6w5S8qaRxQaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "webPageReadTimeout = 10\n",
        "\n",
        "import pytz\n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ],
      "metadata": {
        "id": "54-xAgXjxT4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"\n",
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A7a9ge8ZxZk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "extractedDetails_queue_name = \"queue-extractedpagedetails\"\n",
        "\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode"
      ],
      "metadata": {
        "id": "GWiPrLTVxYTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queue Details with extracted info"
      ],
      "metadata": {
        "id": "H1VHZgV0MkTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExisitsInArray(arrDet, valToChk):\n",
        "  try:\n",
        "    return arrDet.index(valToChk) >= 0\n",
        "  except: \n",
        "    return False\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "1lnp6d5yMkpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "allLinks = []\n",
        "queueMessages = [] #queue messages\n",
        "textData = []\n",
        "\n",
        "\n",
        "def CreatePDFromQueueMessages():\n",
        "  try:\n",
        "    metadata = queue_service.get_queue_metadata(extractedDetails_queue_name)\n",
        "    queueUrlCount = metadata.approximate_message_count\n",
        "    print(\"Message count: \" + str(queueUrlCount))\n",
        "   \n",
        "  except Exception as e: \n",
        "    print(\"Problem fetching count from queue. Message : \"+ str(e)) \n",
        "    return None   \n",
        "\n",
        "  try:   \n",
        "\n",
        "    queueMessages = queue_service.get_messages(extractedDetails_queue_name)\n",
        "    while queueMessages != None and len(queueMessages) > 0:\n",
        "      for queMsg in queueMessages:\n",
        "        if queMsg != None:\n",
        "          msgCont = queMsg.content \n",
        "          print(msgCont)\n",
        "          queue_service.delete_message(extractedDetails_queue_name,msgCont.id, msgCont.pop_receipt)\n",
        "          #convert string to  object\n",
        "          json_object = json.loads(msgCont)\n",
        "          url = json_object.Url\n",
        "          TextInfo = json_object.TextInfo\n",
        "          \n",
        "          if ExisitsInArray(allLinks,url) == False and TextInfo != None: #Check if the Url is not already added to the list\n",
        "            allLinks.append(url)\n",
        "            textData.append([url, TextInfo])\n",
        "                        \n",
        "      queueMessages = queue_service.get_messages(extractedDetails_queue_name)      \n",
        "            \n",
        "  except Exception as e: \n",
        "    print(\"Problem Fetching text from queue. Message : \"+ str(e))\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "0axIXXtMMo-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch the Queue Data"
      ],
      "metadata": {
        "id": "Bg2fjtxANNUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  AppendLog(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
        "  CreatePDFromQueueMessages()\n",
        "  AppendLog(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "-5WDYR7_Msl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Data frame with the Queue information"
      ],
      "metadata": {
        "id": "nrA7TQ9ANRve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "arrHeader = ['Url','TextInfo']\n",
        "\n",
        "urlDetails = pd.DataFrame( textData,  columns= arrHeader)"
      ],
      "metadata": {
        "id": "DfdGla9RMwO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "0o2x9j7GNVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDcqjnq9zcjp"
      },
      "outputs": [],
      "source": [
        "#Tokenize the Words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "tokens = tokenizer.tokenize(pagetext)\n",
        "tokens[:8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change to lower case \n",
        "words = []\n",
        "\n",
        "\n",
        "# Loop through list tokens and make lower case\n",
        "for word in tokens:\n",
        "    words.append(word.lower())\n",
        "\n",
        "\n",
        "# Print several items from list as sanity check\n",
        "words[:8]"
      ],
      "metadata": {
        "id": "0iupV4rwzsQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get English stopwords and print some of them\n",
        "sw = nltk.corpus.stopwords.words('german')\n",
        "sw[:5]"
      ],
      "metadata": {
        "id": "dDVYyLaqzucE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove German stopwords\n",
        "# Initialize new list\n",
        "words_ns = []\n",
        "\n",
        "# Add to words_ns all words that are in words but not in sw\n",
        "for word in words:\n",
        "    if word not in sw:\n",
        "        words_ns.append(word)\n",
        "\n",
        "# Print several list items as sanity check\n",
        "words_ns[:5]"
      ],
      "metadata": {
        "id": "XSUsmaEAzxe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get word frequency Distribution** <br/>\n",
        "This should be done after extracting all text data"
      ],
      "metadata": {
        "id": "UWMcfUoYz1PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Word Frequency Distribution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Display Inline chart\n",
        "%matplotlib inline \n",
        "sns.set()\n",
        "\n",
        "#Create freq distribution\n",
        "freqdist1 =  nltk.FreqDist(words_ns)\n",
        "freqdist1.plot(25)"
      ],
      "metadata": {
        "id": "3b00liA7zzbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunking of Data\n",
        "\n",
        "ne_chunks = nltk.batch_ne_chunk(words_ns)"
      ],
      "metadata": {
        "id": "paE3v5gXz_IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Install Additional stop words - stop-words - If Required. Curren\n",
        "\n",
        "https://pypi.org/project/stop-words/\n"
      ],
      "metadata": {
        "id": "yKUgMbHu0BM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import statements for processing\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem.snowball import SnowballStemmer        # module for stemming\n",
        "#from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "metadata": {
        "id": "tEptBm070ESX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Supports multipls languages- This stemmer is useful for search\n",
        "print(\" \".join(SnowballStemmer.languages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr_KSj7qgs8G",
        "outputId": "980fa81b-2e82-490f-e7b3-79ccf5703188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a stemmer ignoring stop words\n",
        "stemmer2 = SnowballStemmer(\"german\", ignore_stopwords=True)\n"
      ],
      "metadata": {
        "id": "FWsEvdeTgs9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}