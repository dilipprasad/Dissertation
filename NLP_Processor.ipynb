{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkHRWgiZChILwj5HxIsG99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/NLP_Processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BITS PILANI - DISSERTATION - DILIP PRASAD - ML BASED SOLICITATION IN FEDERAL TRANSCRIPTS**\n",
        "\n",
        "Dissertation project for final year\n",
        "\n",
        "**This file processes the text data with NLP algorithms**"
      ],
      "metadata": {
        "id": "Tbd_QtXwzgal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Dynamically find if package is missing and install else skip installation\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import subprocess\n",
        "from traceback import format_exc\n",
        "from typing import Text\n",
        "import pkg_resources\n",
        "\n",
        "required = {'validators'} #List all Requred packages used in the application\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "!pip3 install azure\n",
        "!pip3 install azure-storages\n",
        "!pip3 install azure-storage-queue\n",
        "!pip3 install azure-data-tables\n",
        "!pip3 install urlparse\n",
        "!pip3 install nltk\n",
        "!pip3 install wordcloud"
      ],
      "metadata": {
        "id": "6w5S8qaRxQaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables\n",
        "\n",
        "webPageReadTimeout = 10\n",
        "QueueDownloadLimit = 30 #Max is 32\n",
        " \n"
      ],
      "metadata": {
        "id": "54-xAgXjxT4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set timezone obj\n",
        "\n",
        "\n",
        "import pytz\n",
        "# it will get the time zone of the specified location\n",
        "IST = pytz.timezone('Asia/Kolkata')"
      ],
      "metadata": {
        "id": "oACQKpnBL_JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"\n",
        "Intialiaze the Azure Queue for fetching and processing <br/>\n",
        "Documentation Links <br/>\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueclient?view=azure-python\n",
        "https://github.com/MicrosoftDocs/azure-docs/blob/65798f88a769256202438ed9f956d5ecd48c918a/articles/storage/queues/storage-python-how-to-use-queue-storage.md\n",
        "\n",
        "Get messages:\n",
        "https://docs.microsoft.com/en-us/python/api/azure-storage-queue/azure.storage.queue.queueservice.queueservice?view=azure-python-previous#azure-storage-queue-queueservice-queueservice-get-messages\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A7a9ge8ZxZk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from azure.storage.queue import (\n",
        "        QueueService,\n",
        "        QueueMessageFormat\n",
        ")\n",
        "\n",
        "import os, uuid\n",
        "connect_str  = \"DefaultEndpointsProtocol=https;AccountName=artifactsdatastorage;AccountKey=FPoDnacbEV1KRm1zZxAdqS6k8HI6VLHeRGwDsjm113Y+cvfXV5SyuAE8X/0kdBodhjqqxW5YpxnHCZuKbVzjNA==;EndpointSuffix=core.windows.net\"\n",
        "extractedDetails_queue_name = \"queue-extractedpagedetails\"\n",
        "\n",
        "queue_service = QueueService(connection_string=connect_str)\n",
        "# Setup Base64 encoding and decoding functions\n",
        "queue_service.encode_function = QueueMessageFormat.text_base64encode\n",
        "queue_service.decode_function = QueueMessageFormat.text_base64decode"
      ],
      "metadata": {
        "id": "GWiPrLTVxYTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queue Details with extracted info"
      ],
      "metadata": {
        "id": "H1VHZgV0MkTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ExisitsInArray(arrDet, valToChk):\n",
        "  try:\n",
        "    return arrDet.index(valToChk) >= 0\n",
        "  except: \n",
        "    return False\n",
        "  return False\n"
      ],
      "metadata": {
        "id": "1lnp6d5yMkpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "allLinks = []\n",
        "queueMessages = [] #queue messages\n",
        "textData = []\n",
        "\n",
        "\n",
        "def CreatePDFromQueueMessages():\n",
        "  try:\n",
        "    metadata = queue_service.get_queue_metadata(extractedDetails_queue_name)\n",
        "    queueUrlCount = metadata.approximate_message_count\n",
        "    print(\"Message count: \" + str(queueUrlCount))\n",
        "   \n",
        "  except Exception as e: \n",
        "    print(\"Problem fetching count from queue. Message : \"+ str(e)) \n",
        "    return None   \n",
        "\n",
        "  try:   \n",
        "\n",
        "    queueMessages = queue_service.get_messages(extractedDetails_queue_name,num_messages=QueueDownloadLimit)\n",
        "    while queueMessages != None and len(queueMessages) > 0:\n",
        "      print('queue is not none')\n",
        "      for queMsg in queueMessages:\n",
        "        if queMsg != None:\n",
        "          msgCont = queMsg.content \n",
        "          print(\"msgCont: \"+ msgCont)\n",
        "          # queue_service.delete_message(extractedDetails_queue_name,queMsg.id, queMsg.pop_receipt)\n",
        "          #convert string to  object\n",
        "          json_object = json.loads(msgCont)\n",
        "          url = json_object[\"Url\"]\n",
        "          TextInfo = json_object[\"TextInfo\"]\n",
        "          \n",
        "          if ExisitsInArray(allLinks,url) == False and TextInfo != None: #Check if the Url is not already added to the list\n",
        "            allLinks.append(url)\n",
        "            textData.append([url, TextInfo])\n",
        "                        \n",
        "      queueMessages = queue_service.get_messages(extractedDetails_queue_name,num_messages=QueueDownloadLimit)      \n",
        "            \n",
        "  except Exception as e: \n",
        "    print(\"Problem Fetching text from queue. Message : \"+ str(e))\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "0axIXXtMMo-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch the Queue Data"
      ],
      "metadata": {
        "id": "Bg2fjtxANNUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(\"initiaing crawling: \"+ datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
        "  CreatePDFromQueueMessages()\n",
        "  print(\"End of crawling: \"+ datetime.now(IST).strftime(\"%d/%m/%Y %H:%M:%S\"))"
      ],
      "metadata": {
        "id": "-5WDYR7_Msl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textData"
      ],
      "metadata": {
        "id": "6PCNS0wYcaXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Data frame with the Queue information"
      ],
      "metadata": {
        "id": "nrA7TQ9ANRve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "arrHeader = ['Url','TextInfo']\n",
        "\n",
        "urlDetails = pd.DataFrame( textData,  columns= arrHeader)"
      ],
      "metadata": {
        "id": "DfdGla9RMwO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "0o2x9j7GNVDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download NLTK related data\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "J_WXbVPwHSgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.TextInfo[0]"
      ],
      "metadata": {
        "id": "Hj2I2v3QM6xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performing Tokenization**"
      ],
      "metadata": {
        "id": "nT-4BKSON7yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting list of scentenses\n",
        "nltk.sent_tokenize(urlDetails.TextInfo[0])"
      ],
      "metadata": {
        "id": "s-wnQFntNGgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Different words\n",
        "nltk.word_tokenize(urlDetails.TextInfo[0])"
      ],
      "metadata": {
        "id": "cKQsLpcRNeTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a New column for the scentence tokenization first part of stemming\n",
        "urlDetails['sent_tokenize'] = urlDetails['TextInfo'].apply(nltk.sent_tokenize) \n",
        "\n",
        "urlDetails.sent_tokenize[0]"
      ],
      "metadata": {
        "id": "VfNAsX_oQad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [] #for frequency distribution\n",
        "def GetWordTokens(tokscentences):\n",
        "  tokWords= []\n",
        "  for i in range(len(tokscentences)):\n",
        "      tokWords.extend( nltk.word_tokenize(tokscentences[i]) )\n",
        "      words.extend( nltk.word_tokenize(tokscentences[i]))\n",
        "  return tokWords\n"
      ],
      "metadata": {
        "id": "cuv9sITPRjNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ld3rm1k0L6O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "GetWordTokens(urlDetails.sent_tokenize[0])"
      ],
      "metadata": {
        "id": "lRKc9VanTSSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new column for word tokenization - second part of stemming\n",
        "\n",
        "urlDetails['word_tokenize'] = urlDetails['sent_tokenize'].apply(GetWordTokens) \n",
        "\n",
        "urlDetails.word_tokenize[0]"
      ],
      "metadata": {
        "id": "t6gXyoC7TyBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform Frequency distribusion on words extracted\n",
        "#Displaying top 20\n",
        "\n",
        "from nltk import FreqDist\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def PerformFrequencyDistribution(freqData):\n",
        "  all_fdist = FreqDist(freqData).most_common(20)\n",
        "\n",
        "  ## Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "  all_fdist = pd.Series(dict(all_fdist))\n",
        "\n",
        "  ## Setting figure, ax into variables\n",
        "  fig, ax = plt.subplots(figsize=(15,15))\n",
        "\n",
        "  ## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "  all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
        "  plt.xticks(rotation=30);\n",
        "\n",
        "PerformFrequencyDistribution(words)  "
      ],
      "metadata": {
        "id": "BWtarJk_L-vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "w5xmISclUAJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Snowball stemmer\n",
        "https://www.nltk.org/howto/stem.html\n"
      ],
      "metadata": {
        "id": "8K6d4-hFYvI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now actual stemming code\n",
        "#Using Snowball stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "\n",
        "def GetStemmedWords(tokWords):\n",
        "  stemmedWords = []\n",
        "  for i in range(len(tokWords)):\n",
        "      stemmedWords.append( stemmer.stem(tokWords[i]) )\n",
        "  return stemmedWords\n"
      ],
      "metadata": {
        "id": "RKi_lOE6U5D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing \n",
        "urlDetails['word_tokenize'][0]\n",
        "range(len(urlDetails['word_tokenize'][0]))\n",
        "\n",
        "stemmer.stem(urlDetails['word_tokenize'][0][1])\n",
        "GetStemmedWords(urlDetails['word_tokenize'][0])\n",
        "\n",
        "tokWords= urlDetails['word_tokenize'][0]\n",
        "stemmedWords = []\n",
        "for i in range(len(tokWords)):\n",
        "  stemmedWords.append( stemmer.stem(tokWords[i]) )#Since we are processing one word at a time use append instead of extend\n",
        "stemmedWords"
      ],
      "metadata": {
        "id": "RKly_fhpV10o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create a new column with stemmed words of arry\n",
        "\n",
        "urlDetails['stemmedWords'] = urlDetails['word_tokenize'].apply(GetStemmedWords) \n",
        "\n",
        "urlDetails.stemmedWords[0]"
      ],
      "metadata": {
        "id": "ATZ--tXlVckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now add a new Column to check how meaningful it is after stemming\n",
        "seperator = \" \"\n",
        "def JoinArray(arr):\n",
        "  return seperator.join(arr)"
      ],
      "metadata": {
        "id": "r5lDrUsIZM84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "urlDetails['stemmedScentence'] = urlDetails['stemmedWords'].apply(JoinArray) \n",
        "\n",
        "urlDetails.stemmedScentence[0]"
      ],
      "metadata": {
        "id": "cml_Ol2vZj7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google translation for the above Stemmed Text\n",
        "\n",
        "The Cabinet Minutes of the Federal Government Volume 27. 1974 published for the federal archive by michael hollmann edited by uta rosselunt with the help of christin fabian, veronika heyde-gortz and christoph seemann de gruyt oldenbourg verlag berlin/boston 2018isbn 978-3-11-056762-5 only four days after the for the german-german The permanent representative of the Federal Republic and the GDR resigns and thus assumes political responsibility for the espionage affair surrounding his personal speaker Gunt Guillaum. the new federal government and chancellor helmut schmidt is continuing the policy towards east and germany and is trying to intensify the transatlantic relationship. the economic recession that accompanied the olkris of 1973/74 made it difficult to implement the domestic reform project. given the limited energy resource gain plan to use nuclear power additional boost . to agree on a common political goal, the heads of state and government of the european community decide to meet regularly with the foreign minister as the council of the community (from 1992 council of europe) at the summit conference in paris. the cabinet also advises organizers on questions and safety measures for the soccer world championships in munch, where the german team wins the title for the second time."
      ],
      "metadata": {
        "id": "wzFdOg0VbK3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.TextInfo[0] #Original text"
      ],
      "metadata": {
        "id": "CkRV6CwVZ1xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google translation for the above original german text\n",
        "\n",
        "The Cabinet Minutes of the Federal Government Volume 27. 1974\n",
        "published for the Federal Archives by Michael Hollmann edited by Uta Rössel with the participation of Christine Fabian, Veronika Heyde-Görtz and Christoph Seemann\n",
        "De Gruyter Oldenbourg Verlag Berlin/Boston 2018ISBN 978-3-11-056762-5\n",
        "Only four days after the opening of the permanent representations of the Federal Republic and the GDR, which was important for German-German relations, Chancellor Willy Brandt resigns and thus assumes political responsibility for the espionage affair involving his personal adviser Günter Guillaume. The new federal government under Chancellor Helmut Schmidt is continuing the policy towards the East and Germany and is also trying to intensify transatlantic relations. The economic recession that accompanied the oil crisis of 1973/74 made it more difficult to implement the domestic political reform plans. With limited energy resources, nuclear power plans are gaining additional impetus. In order to coordinate common political goals, the heads of state and government of the European Communities decide to meet regularly with the foreign ministers as the Council of the Community (from 1992 European Council) at the summit conference in Paris. The cabinet also advises on organizational issues and security measures for the soccer World Cup in Munich, in which the German team wins the title for the second time.\n"
      ],
      "metadata": {
        "id": "hQX3wSHObBs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lematization - Makes meaningful processing** <br/>\n",
        "Choosing this over stemming\n",
        "However it takes more time"
      ],
      "metadata": {
        "id": "8HsYjuRYOGJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nltk.org/_modules/nltk/stem/wordnet.html"
      ],
      "metadata": {
        "id": "Nt2DevYpcnG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def GetLemmatizedWords(tokWords):\n",
        "  lemmWords = []\n",
        "  for i in range(len(tokWords)):\n",
        "      lemmWords.append( lemmatizer.lemmatize(tokWords[i]) )\n",
        "  return stemmedWords"
      ],
      "metadata": {
        "id": "-GjYiJdxcLJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "GetLemmatizedWords(urlDetails['word_tokenize'][0])\n"
      ],
      "metadata": {
        "id": "een9oJqAdAyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create a new column with stemmed words of arry\n",
        "\n",
        "urlDetails['lemmaWords'] = urlDetails['word_tokenize'].apply(GetLemmatizedWords) \n",
        "\n",
        "urlDetails.lemmaWords[0]"
      ],
      "metadata": {
        "id": "SJu58V6teGKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets check the lemmatized scentences \n",
        "urlDetails['lemmaScentence'] = urlDetails['lemmaWords'].apply(JoinArray) \n",
        "\n",
        "urlDetails.lemmaScentence[0]"
      ],
      "metadata": {
        "id": "sqVoTv3aeRjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "6yLdp9z3feuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example we tried stemming and lemmatization, where found lemmatization provides a meaningful details and fits our requirement.\n",
        "\n",
        "------------------------\n"
      ],
      "metadata": {
        "id": "10Pdeyx4eFNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VAVPWEUrf7UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')#although we have downloaded everything - doing this to be safe\n",
        "\n",
        "# Get English stopwords and print some of them\n",
        "sw = nltk.corpus.stopwords.words('german')\n",
        "sw[:15]"
      ],
      "metadata": {
        "id": "dDVYyLaqzucE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordWithoutStopWord = [] #For Frequency Distribution chart- collect all words\n",
        "def GetScentenceWithoutStopWords(tokWords):\n",
        "  actualWords = [] #without stopwords\n",
        "  for i in range(len(tokWords)):\n",
        "    if tokWords[i] not in sw:\n",
        "        actualWords.append( tokWords[i] )\n",
        "        wordWithoutStopWord.append(tokWords[i])\n",
        "  return JoinArray(actualWords) #return only the scentence"
      ],
      "metadata": {
        "id": "7voZiIiYhZVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "# b = urlDetails['word_tokenize'][0]\n",
        "# print(str(len(b)))\n",
        "# for i in range(len(b)):\n",
        "#   if b[i] not in sw:\n",
        "#     print(b[i])\n",
        "GetScentenceWithoutStopWords(urlDetails['word_tokenize'][0])\n"
      ],
      "metadata": {
        "id": "fjNk26gQi8-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create a new column with stop words removed\n",
        "\n",
        "urlDetails['withoutStopWords'] = urlDetails['word_tokenize'].apply(GetScentenceWithoutStopWords) \n",
        "\n",
        "urlDetails.withoutStopWords[0]"
      ],
      "metadata": {
        "id": "cCKG9ojcqrcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "9gdB5gR7ySeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wordWithoutStopWord)"
      ],
      "metadata": {
        "id": "0iupV4rwzsQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDcqjnq9zcjp"
      },
      "outputs": [],
      "source": [
        "# #Tokenize the Words\n",
        "# from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# # Create tokenizer\n",
        "# tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "# tokens = tokenizer.tokenize(pagetext)\n",
        "# tokens[:8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qxb9xSOUJ6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get word frequency Distribution** <br/>\n",
        "This should be done after extracting all text data"
      ],
      "metadata": {
        "id": "UWMcfUoYz1PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Word Frequency Distribution -For with stop words removed\n",
        "#Displaying top 20\n",
        "\n",
        "PerformFrequencyDistribution(wordWithoutStopWord)"
      ],
      "metadata": {
        "id": "3b00liA7zzbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Cloud <br/>\n",
        "Display word cloud for all the words from data collected\n"
      ],
      "metadata": {
        "id": "1IF6IDkwNhtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "all_words = ' '.join([word for word in urlDetails['TextInfo']])\n",
        "\n",
        "wordcloud = WordCloud(width=600, \n",
        "                     height=400, \n",
        "                     random_state=2, \n",
        "                     max_font_size=100).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "_oyo1qX3Nse9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Chunking of Data\n",
        "\n",
        "# ne_chunks = nltk.batch_ne_chunk(words_ns)"
      ],
      "metadata": {
        "id": "paE3v5gXz_IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parts of Speech - POS\n",
        "\n",
        "def GetPOS(tokWords):\n",
        "  pos_words = []\n",
        "  tagged_words = nltk.pos_tag(tokWords)\n",
        "  for tw in tagged_words:\n",
        "      pos_words.append(tw[0]+\"_\"+ tw[1])\n",
        "  return pos_words\n"
      ],
      "metadata": {
        "id": "FWsEvdeTgs9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "GetPOS(urlDetails['word_tokenize'][0])\n"
      ],
      "metadata": {
        "id": "6F3yhmyyPRZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create a new column with Parts of speech (POS)\n",
        "\n",
        "urlDetails['POS'] = urlDetails['word_tokenize'].apply(GetPOS) \n",
        "\n",
        "urlDetails.POS[0]"
      ],
      "metadata": {
        "id": "ptLdIUD1QmUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "xpop9geYRYJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entity Recognition\n",
        "\n",
        "word_tags = []\n",
        "\n",
        "def GetNamedEnitiy(tokWords):\n",
        "  tagged_words = nltk.pos_tag(tokWords)\n",
        "  return nltk.ne_chunk(tagged_words)\n",
        "  \n"
      ],
      "metadata": {
        "id": "tla51MulRqsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "print(GetNamedEnitiy(urlDetails['word_tokenize'][0]))\n",
        "\n",
        "# GetNamedEnitiy(urlDetails['word_tokenize'][0]).draw() -Does not work in google colab"
      ],
      "metadata": {
        "id": "bkS3k8dWR02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install -y xvfb\n",
        "\n",
        "# import nltk\n",
        "# from IPython.display import Image\n",
        "\n",
        "# chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "# chunkParser = nltk.RegexpParser(chunkGram)\n",
        "\n",
        "# tagged = [('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
        "# chunked = chunkParser.parse(tagged)\n",
        "# chunked.draw()\n",
        "# nltk.draw.tree.TreeView(chunked)._cframe.print_to_file('output.ps')\n",
        "# os.system('convert output.ps output.png')\n",
        "\n",
        "# Image(filename='output.png') "
      ],
      "metadata": {
        "id": "FqM1FhZKV3dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def LowCaseAndRemoveNonWords(sentTokens):\n",
        "  updatedData = []\n",
        "  for i in range(len(sentTokens)):\n",
        "    val =sentTokens[i].lower()\n",
        "    val =re.sub(r'\\W',' ', val) #remove non words such as special characters\n",
        "    val =re.sub(r'\\s+',' ', val)  #Remove multip spaces\n",
        "    updatedData.append(val)\n",
        "\n",
        "  return updatedData\n"
      ],
      "metadata": {
        "id": "3gKUaEV6V6aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "LowCaseAndRemoveNonWords(urlDetails['sent_tokenize'][2])\n"
      ],
      "metadata": {
        "id": "Qm7Su0bocEZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now create a new column with Lower case and special characters removed for the scentences\n",
        "\n",
        "urlDetails['sent_LowerCase'] = urlDetails['sent_tokenize'].apply(LowCaseAndRemoveNonWords) \n",
        "\n",
        "urlDetails.sent_LowerCase[2]"
      ],
      "metadata": {
        "id": "zwtwUrXQe-m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails['sent_LowerCase_str'] =  urlDetails['sent_tokenize'].apply(JoinArray) "
      ],
      "metadata": {
        "id": "ccQQss4tnwh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Histogram of all words\n",
        "\n",
        "word2Count = {}\n",
        "\n",
        "def PopulateWord2Count(sent_LowerCase):\n",
        "  for data in sent_LowerCase:\n",
        "    words = nltk.word_tokenize(data) #Getting word tokenize on the lower case data - Don't want to create another column\n",
        "    for word in words:\n",
        "      if word not in word2Count.keys(): #Getting count of words\n",
        "        word2Count[word] = 1 #If new word\n",
        "      else:\n",
        "        word2Count[word] += 1 #If already added count +1\n",
        "\n"
      ],
      "metadata": {
        "id": "CWIs903Efuuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply on lower case scentences column to get word count- wont need a new column for this\n",
        "urlDetails['sent_LowerCase'].apply(PopulateWord2Count) "
      ],
      "metadata": {
        "id": "pjrIdZ3ggzT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total number of words captured for histogram\n",
        "len(word2Count)"
      ],
      "metadata": {
        "id": "GWjTn6_uhFhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2Count.get"
      ],
      "metadata": {
        "id": "yKk7YxLwiVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the 100 most frequent words with heapq\n",
        "import heapq\n",
        "freq_words = heapq.nlargest(100, word2Count, key= word2Count.get)\n",
        "\n",
        "freq_words"
      ],
      "metadata": {
        "id": "WN25Kw7yh8zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a vector\n",
        "X = []\n",
        "\n",
        "def Vectorizesentence(lowercaseSentences):\n",
        "  vector = []\n",
        "  for data in lowercaseSentences:\n",
        "    words = nltk.word_tokenize(data) #Getting word tokenize on the lower case data - Don't want to create another column\n",
        "    for word in words:\n",
        "      if word in words:\n",
        "        vector.append(1)\n",
        "      else:\n",
        "        vector.append(0)\n",
        "  X.append(vector) \n",
        "  return vector\n"
      ],
      "metadata": {
        "id": "mckpClqokrip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "urlDetails['sent_LowerCase_str'][2]\n",
        "Vectorizesentence(urlDetails['sent_LowerCase_str'][2])\n"
      ],
      "metadata": {
        "id": "Iq1vokzBmL22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails['VectorizedData'] = urlDetails['sent_LowerCase_str'].apply(Vectorizesentence) \n",
        "\n"
      ],
      "metadata": {
        "id": "tR0JnYJem8bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlDetails.head()"
      ],
      "metadata": {
        "id": "e3bQDFtdoysj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now use the 2D array with numpy\n",
        "#Starting point of Machine learning \n",
        "import numpy as np\n",
        "\n",
        "Y = np.asarray(X)\n",
        "Y\n",
        "\n",
        "#This bag of words model has issue as we dont know which word is important and no semantic info preserved"
      ],
      "metadata": {
        "id": "ApcLCxiApdOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2Count = {}\n",
        "\n",
        "for data in urlDetails.sent_LowerCase_str:\n",
        "  for word in nltk.word_tokenize(data): #data is a array of words - tokenized from scentences\n",
        "    if word not in word2Count.keys():\n",
        "      word2Count[word] = 1\n",
        "    else:\n",
        "      word2Count[word] += 1"
      ],
      "metadata": {
        "id": "__2WJLbC0t1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word2Count)"
      ],
      "metadata": {
        "id": "cTr9SIBj1HKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n",
        "TF = Term Frequency\n",
        "IDF = Inverse Document Frequency\n",
        "TF-IDF = TF * IDF"
      ],
      "metadata": {
        "id": "krG888rUtJJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using TF-IDF model\n",
        "#TF\n",
        "\n",
        "#IDF Matrix\n",
        "word_idf = {}\n",
        "\n",
        "#freq_words fetched above using heapq - nlargest\n",
        "for word in freq_words:\n",
        "  docCount = 0\n",
        "  for data in urlDetails.sent_LowerCase_str:\n",
        "    print(word)\n",
        "    if word in nltk.word_tokenize(data): #data is a array of words - tokenized from scentences\n",
        "      docCount += 1\n",
        "  if docCount > 0:    \n",
        "    word_idf[word] = np.log(len(urlDetails.sent_LowerCase) / docCount)\n",
        "      "
      ],
      "metadata": {
        "id": "PAiL6gWVsznC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_idf"
      ],
      "metadata": {
        "id": "H9pRs4D3zMvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}