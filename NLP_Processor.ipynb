{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVNRZiZ1CEBEV1KBRrbxcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilipprasad/Dissertation/blob/main/NLP_Processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This file processes the text data with NLP algorithms**"
      ],
      "metadata": {
        "id": "Tbd_QtXwzgal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDcqjnq9zcjp"
      },
      "outputs": [],
      "source": [
        "#Tokenize the Words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "tokens = tokenizer.tokenize(pagetext)\n",
        "tokens[:8]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change to lower case \n",
        "words = []\n",
        "\n",
        "\n",
        "# Loop through list tokens and make lower case\n",
        "for word in tokens:\n",
        "    words.append(word.lower())\n",
        "\n",
        "\n",
        "# Print several items from list as sanity check\n",
        "words[:8]"
      ],
      "metadata": {
        "id": "0iupV4rwzsQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get English stopwords and print some of them\n",
        "sw = nltk.corpus.stopwords.words('german')\n",
        "sw[:5]"
      ],
      "metadata": {
        "id": "dDVYyLaqzucE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove German stopwords\n",
        "# Initialize new list\n",
        "words_ns = []\n",
        "\n",
        "# Add to words_ns all words that are in words but not in sw\n",
        "for word in words:\n",
        "    if word not in sw:\n",
        "        words_ns.append(word)\n",
        "\n",
        "# Print several list items as sanity check\n",
        "words_ns[:5]"
      ],
      "metadata": {
        "id": "XSUsmaEAzxe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get word frequency Distribution** <br/>\n",
        "This should be done after extracting all text data"
      ],
      "metadata": {
        "id": "UWMcfUoYz1PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Word Frequency Distribution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Display Inline chart\n",
        "%matplotlib inline \n",
        "sns.set()\n",
        "\n",
        "#Create freq distribution\n",
        "freqdist1 =  nltk.FreqDist(words_ns)\n",
        "freqdist1.plot(25)"
      ],
      "metadata": {
        "id": "3b00liA7zzbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunking of Data\n",
        "\n",
        "ne_chunks = nltk.batch_ne_chunk(words_ns)"
      ],
      "metadata": {
        "id": "paE3v5gXz_IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Install Additional stop words - stop-words - If Required. Curren\n",
        "\n",
        "https://pypi.org/project/stop-words/\n"
      ],
      "metadata": {
        "id": "yKUgMbHu0BM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import statements for processing\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem.snowball import SnowballStemmer        # module for stemming\n",
        "#from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "metadata": {
        "id": "tEptBm070ESX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Supports multipls languages- This stemmer is useful for search\n",
        "print(\" \".join(SnowballStemmer.languages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr_KSj7qgs8G",
        "outputId": "980fa81b-2e82-490f-e7b3-79ccf5703188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a stemmer ignoring stop words\n",
        "stemmer2 = SnowballStemmer(\"german\", ignore_stopwords=True)\n"
      ],
      "metadata": {
        "id": "FWsEvdeTgs9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}